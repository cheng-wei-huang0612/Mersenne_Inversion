(*
cv -v -vo lex -jobs 64 -isafety lab/divstep_bunch.cl
Parsing CryptoLine file:			[OK]		0.0095 seconds
Checking well-formedness:			[OK]		0.0046 seconds

Procedure divstep_ver_a
=======================
Transforming to SSA form:			[OK]		0.0000 seconds
Normalizing specification:			[OK]		0.0000 seconds
Rewriting assignments:				[OK]		0.0000 seconds
Verifying program safety:			
	Cut 0
	     Round 1 (2 safety conditions, timeout = 300 seconds)
		Safety condition #1		[OK]
		Safety condition #0		[OK]
	Overall					[OK]		0.0053 seconds
Verifying range assertions:			[OK]		0.0866 seconds
Verifying range specification:			[OK]		0.0066 seconds
Rewriting value-preserved casting:		[OK]		0.0000 seconds
Verifying algebraic assertions:			[OK]		0.0000 seconds
Verifying algebraic specification:		[OK]		0.0000 seconds

Procedure Summary
-----------------
Procedure verification:				[OK]		0.0986 seconds

Procedure divstep_ver_b
=======================
Transforming to SSA form:			[OK]		0.0000 seconds
Normalizing specification:			[OK]		0.0000 seconds
Rewriting assignments:				[OK]		0.0000 seconds
Verifying program safety:			
	Cut 0
	     Round 1 (2 safety conditions, timeout = 300 seconds)
		Safety condition #1		[OK]
		Safety condition #0		[OK]
	Overall					[OK]		0.0034 seconds
Verifying range assertions:			[OK]		0.0865 seconds
Verifying range specification:			[OK]		0.0065 seconds
Rewriting value-preserved casting:		[OK]		0.0000 seconds
Verifying algebraic assertions:			[OK]		0.0000 seconds
Verifying algebraic specification:		[OK]		0.0000 seconds

Procedure Summary
-----------------
Procedure verification:				[OK]		0.0966 seconds

Procedure divstep_ver_c
=======================
Transforming to SSA form:			[OK]		0.0000 seconds
Normalizing specification:			[OK]		0.0000 seconds
Rewriting assignments:				[OK]		0.0000 seconds
Verifying program safety:			
	Cut 0
	     Round 1 (2 safety conditions, timeout = 300 seconds)
		Safety condition #1		[OK]
		Safety condition #0		[OK]
	Overall					[OK]		0.0033 seconds
Verifying range assertions:			[OK]		0.0896 seconds
Verifying range specification:			[OK]		0.0062 seconds
Rewriting value-preserved casting:		[OK]		0.0000 seconds
Verifying algebraic assertions:			[OK]		0.0000 seconds
Verifying algebraic specification:		[OK]		0.0000 seconds

Procedure Summary
-----------------
Procedure verification:				[OK]		0.0993 seconds

Procedure divstepx19_3
======================
Transforming to SSA form:			[OK]		0.0001 seconds
Normalizing specification:			[OK]		0.0002 seconds
Rewriting assignments:				[OK]		0.0001 seconds
Verifying program safety:			
	Cut 0
	     Round 1 (6 safety conditions, timeout = 300 seconds)
		Safety condition #2		[OK]
		Safety condition #5		[OK]
		Safety condition #1		[OK]
		Safety condition #4		[OK]
		Safety condition #0		[OK]
		Safety condition #3		[OK]
	Overall					[OK]		0.0554 seconds
Verifying range assertions:			[OK]		0.2705 seconds
Verifying range specification:			[OK]		0.1000 seconds
Rewriting value-preserved casting:		[OK]		0.0000 seconds
Verifying algebraic assertions:			[OK]		0.0078 seconds
Verifying algebraic specification:		[OK]		0.0064 seconds

Procedure Summary
-----------------
Procedure verification:				[OK]		0.4409 seconds

Procedure divstepx20_1
======================
Transforming to SSA form:			[OK]		0.0001 seconds
Normalizing specification:			[OK]		0.0002 seconds
Rewriting assignments:				[OK]		0.0001 seconds
Verifying program safety:			
	Cut 0
	     Round 1 (6 safety conditions, timeout = 300 seconds)
		Safety condition #2		[OK]
		Safety condition #5		[OK]
		Safety condition #4		[OK]
		Safety condition #1		[OK]
		Safety condition #0		[OK]
		Safety condition #3		[OK]
	Overall					[OK]		0.0523 seconds
Verifying range assertions:			[OK]		0.2788 seconds
Verifying range specification:			[OK]		0.3365 seconds
Rewriting value-preserved casting:		[OK]		0.0000 seconds
Verifying algebraic assertions:			[OK]		0.0097 seconds
Verifying algebraic specification:		[OK]		0.0047 seconds

Procedure Summary
-----------------
Procedure verification:				[OK]		0.6827 seconds

Procedure divstepx20_2
======================
Transforming to SSA form:			[OK]		0.0002 seconds
Normalizing specification:			[OK]		0.0002 seconds
Rewriting assignments:				[OK]		0.0003 seconds
Verifying program safety:			
	Cut 0
	     Round 1 (6 safety conditions, timeout = 300 seconds)
		Safety condition #2		[OK]
		Safety condition #5		[OK]
		Safety condition #4		[OK]
		Safety condition #1		[OK]
		Safety condition #0		[OK]
		Safety condition #3		[OK]
	Overall					[OK]		0.0561 seconds
Verifying range assertions:			[OK]		0.5559 seconds
Verifying range specification:			[OK]		0.1947 seconds
Rewriting value-preserved casting:		[OK]		0.0001 seconds
Verifying algebraic assertions:			[OK]		0.0168 seconds
Verifying algebraic specification:		[OK]		0.0128 seconds

Procedure Summary
-----------------
Procedure verification:				[OK]		0.8372 seconds

Procedure main
==============
Transforming to SSA form:			[OK]		0.0001 seconds
Normalizing specification:			[OK]		0.0001 seconds
Rewriting assignments:				[OK]		0.0000 seconds
Verifying program safety:			
	Cut 0
	     Round 1 (0 safety conditions, timeout = 300 seconds)
	Overall					[OK]		0.0000 seconds
Verifying range assertions:			[OK]		0.0194 seconds
Verifying range specification:			[OK]		0.0000 seconds
Rewriting value-preserved casting:		[OK]		0.0000 seconds
Verifying algebraic assertions:			[OK]		0.0053 seconds
Verifying algebraic specification:		[OK]		0.0000 seconds

Procedure Summary
-----------------
Procedure verification:				[OK]		0.0253 seconds

Procedure update_uuvvrrss
=========================
Transforming to SSA form:			[OK]		0.0000 seconds
Normalizing specification:			[OK]		0.0000 seconds
Rewriting assignments:				[OK]		0.0000 seconds
Verifying program safety:			
	Cut 0
	     Round 1 (12 safety conditions, timeout = 300 seconds)
		Safety condition #2		[OK]
		Safety condition #1		[OK]
		Safety condition #11		[OK]
		Safety condition #10		[OK]
		Safety condition #5		[OK]
		Safety condition #8		[OK]
		Safety condition #4		[OK]
		Safety condition #6		[OK]
		Safety condition #0		[OK]
		Safety condition #3		[OK]
		Safety condition #7		[OK]
		Safety condition #9		[OK]
	Overall					[OK]		85.9783 seconds
Verifying range assertions:			[OK]		0.0024 seconds
Verifying range specification:			[OK]		0.0025 seconds
Rewriting value-preserved casting:		[OK]		0.0000 seconds
Verifying algebraic assertions:			[OK]		0.0047 seconds
Verifying algebraic specification:		[OK]		0.0040 seconds

Procedure Summary
-----------------
Procedure verification:				[OK]		85.9921 seconds

Procedure update_uuvvrrss_2
===========================
Transforming to SSA form:			[OK]		0.0000 seconds
Normalizing specification:			[OK]		0.0000 seconds
Rewriting assignments:				[OK]		0.0000 seconds
Verifying program safety:			
	Cut 0
	     Round 1 (12 safety conditions, timeout = 300 seconds)
		Safety condition #0		[OK]
		Safety condition #6		[OK]
		Safety condition #7		[OK]
		Safety condition #10		[OK]
		Safety condition #8		[OK]
		Safety condition #1		[OK]
		Safety condition #4		[OK]
		Safety condition #3		[OK]
		Safety condition #9		[OK]
		Safety condition #11		[OK]
		Safety condition #5		[TIMEOUT]
		Safety condition #2		[TIMEOUT]
	     Round 2 (2 safety conditions, timeout = 600 seconds)
		Safety condition #2		[TIMEOUT]
		Safety condition #5		[OK]
	     Round 3 (1 safety conditions, timeout = 1200 seconds)
		Safety condition #2		[OK]
	Overall					[OK]		2022.8635 seconds
Verifying range assertions:			[OK]		0.0027 seconds
Verifying range specification:			[OK]		0.0026 seconds
Rewriting value-preserved casting:		[OK]		0.0000 seconds
Verifying algebraic assertions:			[OK]		0.0047 seconds
Verifying algebraic specification:		[OK]		0.0044 seconds

Procedure Summary
-----------------
Procedure verification:				[OK]		2022.8781 seconds

Summary
=======
Verification result:				[OK]		2111.1653 seconds

*)


proc divstep_ver_a(fuv@sint64, grs@sint64, delta@sint64, delta_range@sint64;
                   fuv_out@sint64, grs_out@sint64, delta_out@sint64, ne_out@bit, delta_range_out@sint64) = {
    true && 
    fuv = (const 64 1) (mod (const 64 2)),
    delta_range >=s (const 64 0),
    delta_range <=s (const 64 1400),
    ((const 64 0)-delta_range) <=s delta, delta <=s delta_range
}

mov x8 grs;
mov x7 fuv;
mov x3 delta;


(* tst	x8, #0x1 *)
spl dc x8_lo x8 1;
and ne@bit x8_lo 1@bit;

(* csel	x10, x7, xzr, ne *)
cmov x10 ne x7 0@sint64;

(* ccmp	x3, xzr, #0x8, ne *)
spl ge dc x3 63;
not ge@bit ge;
cmov ge ne ge 0@bit;	// ne = any;

(* cneg	x3, x3, ge *)
subs dc x3_neg 0@sint64 x3;
cmov x3 ge x3_neg x3;	// ge = tcont;;

(* cneg	x10, x10, ge *)
subs dc x10_neg 0@sint64 x10;
cmov x10 ge x10_neg x10;	// ge = tcont;;

(* csel	x7, x8, x7, ge *)
cmov x7 ge x8 x7;

(* add	x8, x8, x10 *)
adds dc x8 x8 x10;

(* add	x3, x3, #0x2 *)
add x3 x3 0x2@sint64;

(* tst	x8, #0x2 *)
spl dc x8_lo x8 2;
spl x8_target dc x8_lo 1;
and ne@bit x8_target 1@bit;

(* asr	x8, x8, #1 *)
ssplit x8 dcL x8 1;

assert true &&
or [
    and [
        grs = (const 64 0) (mod (const 64 2)),

        x7 = fuv,
        x8 * (const 64 2) = grs,
        x3 = (const 64 2) + delta
    ],
    and [
        grs = (const 64 1) (mod (const 64 2)),
        delta <s (const 64 0),

        x7 = fuv,
        x8 * (const 64 2) = grs + fuv,
        x3 = (const 64 2) + delta
    ],
    and [
        grs = (const 64 1) (mod (const 64 2)),
        delta >=s (const 64 0),

        x7 = grs,
        x8 * (const 64 2) = grs - fuv,
        x3 = (const 64 2) - delta
    ]
]
;

mov fuv_out x7;
mov grs_out x8;
mov delta_out x3;
mov ne_out ne;
add delta_range_out delta_range 2@sint64;

{true &&
    fuv_out = (const 64 1) (mod (const 64 2)),
    grs_out = (uext ne_out 63) (mod (const 64 2)),
    (const 64 0) -delta_range_out <=s delta_out, delta_out <=s delta_range_out,
    delta_range_out = delta_range + (const 64 2)
}


proc divstep_ver_b(fuv@sint64, grs@sint64, delta@sint64, ne@bit, delta_range@sint64;
                   fuv_out@sint64, grs_out@sint64, delta_out@sint64, ne_out@bit, delta_range_out@sint64) = {
    true && 
    fuv = (const 64 1) (mod (const 64 2)),
    grs = (uext ne 63) (mod (const 64 2)),
    ((const 64 0)-delta_range) <=s delta, delta <=s delta_range,
    delta_range >=s (const 64 0),
    delta_range <=s (const 64 1400)
}

mov x8 grs;
mov x7 fuv;
mov x3 delta;

(* csel	x10, x7, xzr, ne *)
cmov x10 ne x7 0@sint64;	// ne = any;

(* ccmp	x3, xzr, #0x8, ne *)
spl ge dc x3 63;
not ge@bit ge;
cmov ge ne ge 0@bit;	// ne = any;

(* cneg	x3, x3, ge *)
subs dc x3_neg 0@sint64 x3;
cmov x3 ge x3_neg x3;	// ge = tcont;

(* cneg	x10, x10, ge *)
subs dc x10_neg 0@sint64 x10;
cmov x10 ge x10_neg x10;	// ge = tcont;

(* csel	x7, x8, x7, ge *)
cmov x7 ge x8 x7;	// ge = tcont;

(* add	x8, x8, x10 *)
adds dc x8 x8 x10;

(* add	x3, x3, #0x2 *)
add x3 x3 0x2@sint64;

(* tst	x8, #0x2 *)
spl dc x8_lo x8 2;
spl x8_target dc x8_lo 1;
and ne@bit x8_target 1@bit;

(* asr	x8, x8, #1 *)
split x8 dc x8 1;


assert true &&
or [
    and [
        grs = (const 64 0) (mod (const 64 2)),

        x7 = fuv,
        x8 * (const 64 2) = grs,
        x3 = (const 64 2) + delta
    ],
    and [
        grs = (const 64 1) (mod (const 64 2)),
        delta <s (const 64 0),

        x7 = fuv,
        x8 * (const 64 2) = grs + fuv,
        x3 = (const 64 2) + delta
    ],
    and [
        grs = (const 64 1) (mod (const 64 2)),
        delta >=s (const 64 0),

        x7 = grs,
        x8 * (const 64 2) = grs - fuv,
        x3 = (const 64 2) - delta
    ]
]
;

mov fuv_out x7;
mov grs_out x8;
mov delta_out x3;
mov ne_out ne;
add delta_range_out delta_range 2@sint64;


{true &&
    fuv_out = (const 64 1) (mod (const 64 2)),
    grs_out = (uext ne_out 63) (mod (const 64 2)),
    (const 64 0) -delta_range_out <=s delta_out, delta_out <=s delta_range_out,
    delta_range_out = delta_range + (const 64 2)
}


proc divstep_ver_c(fuv@sint64, grs@sint64, delta@sint64, ne@bit, delta_range@sint64;
                   fuv_out@sint64, grs_out@sint64, delta_out@sint64, delta_range_out@sint64) = {
    true && 
    fuv = (const 64 1) (mod (const 64 2)),
    grs = (uext ne 63) (mod (const 64 2)),
    ((const 64 0)-delta_range) <=s delta, delta <=s delta_range,
    delta_range >=s (const 64 0),
    delta_range <=s (const 64 1400)
}

mov x8 grs;
mov x7 fuv;
mov x3 delta;

(* csel	x10, x7, xzr, ne *)
cmov x10 ne x7 0@sint64;	// ne = any;

(* ccmp	x3, xzr, #0x8, ne *)
spl ge dc x3 63;
not ge@bit ge;
cmov ge ne ge 0@bit;	// ne = any;

(* cneg	x3, x3, ge *)
subs dc x3_neg 0@sint64 x3;
cmov x3 ge x3_neg x3;	// ge = tcont;

(* cneg	x10, x10, ge *)
subs dc x10_neg 0@sint64 x10;
cmov x10 ge x10_neg x10;	// ge = tcont;

(* csel	x7, x8, x7, ge *)
cmov x7 ge x8 x7;	// ge = tcont;

(* add	x8, x8, x10 *)
adds dc x8 x8 x10;

(* add	x3, x3, #0x2 *)
add x3 x3 0x2@sint64;

(* asr	x8, x8, #1 *)
split x8 dc x8 1;


assert true &&
or [
    and [
        grs = (const 64 0) (mod (const 64 2)),

        x7 = fuv,
        x8 * (const 64 2) = grs,
        x3 = (const 64 2) + delta
    ],
    and [
        grs = (const 64 1) (mod (const 64 2)),
        delta <s (const 64 0),

        x7 = fuv,
        x8 * (const 64 2) = grs + fuv,
        x3 = (const 64 2) + delta
    ],
    and [
        grs = (const 64 1) (mod (const 64 2)),
        delta >=s (const 64 0),

        x7 = grs,
        x8 * (const 64 2) = grs - fuv,
        x3 = (const 64 2) - delta
    ]
]
;

mov fuv_out x7;
mov grs_out x8;
mov delta_out x3;
add delta_range_out delta_range 2@sint64;


{true &&
    fuv_out = (const 64 1) (mod (const 64 2)),
    (const 64 0) -delta_range_out <=s delta_out, delta_out <=s delta_range_out,
    delta_range_out = delta_range + (const 64 2)
}

proc update_uuvvrrss (
    u_0_20@sint64,
    v_0_20@sint64,
    r_0_20@sint64,
    s_0_20@sint64,
    u_20_40@sint64,
    v_20_40@sint64,
    r_20_40@sint64,
    s_20_40@sint64;
    u_0_40@sint64,
    v_0_40@sint64,
    r_0_40@sint64,
    s_0_40@sint64
) = 
{
  true
        &&

(const 64 (-(2**20))) <=s u_0_20, u_0_20 <=s (const 64 ((2**20) - 1)),
(const 64 (-(2**20))) <=s u_20_40, u_20_40 <=s (const 64 ((2**20) - 1)),
(const 64 (-(2**20))) <=s v_0_20, v_0_20 <=s (const 64 ((2**20) - 1)),
(const 64 (-(2**20))) <=s v_20_40, v_20_40 <=s (const 64 ((2**20) - 1)),
(const 64 (-(2**20))) <=s r_0_20, r_0_20 <=s (const 64 ((2**20) - 1)),
(const 64 (-(2**20))) <=s r_20_40, r_20_40 <=s (const 64 ((2**20) - 1)),
(const 64 (-(2**20))) <=s s_0_20, s_0_20 <=s (const 64 ((2**20) - 1)),
(const 64 (-(2**20))) <=s s_20_40, s_20_40 <=s (const 64 ((2**20) - 1)),
u_20_40 + v_20_40 <=s (const 64 (2**20)),
u_20_40 - v_20_40 <=s (const 64 (2**20)),
(const 64 0) - u_20_40 + v_20_40 <=s (const 64 (2**20)),
(const 64 0) - u_20_40 - v_20_40 <=s (const 64 (2**20)),
r_20_40 + s_20_40 <=s (const 64 (2**20)),
r_20_40 - s_20_40 <=s (const 64 (2**20)),
(const 64 0) - r_20_40 + s_20_40 <=s (const 64 (2**20)),
(const 64 0) - r_20_40 - s_20_40 <=s (const 64 (2**20)),
u_0_20 + v_0_20 <=s (const 64 (2**20)),
u_0_20 - v_0_20 <=s (const 64 (2**20)),
(const 64 0) - u_0_20 + v_0_20 <=s (const 64 (2**20)),
(const 64 0) - u_0_20 - v_0_20 <=s (const 64 (2**20)),
r_0_20 + s_0_20 <=s (const 64 (2**20)),
r_0_20 - s_0_20 <=s (const 64 (2**20)),
(const 64 0) - r_0_20 + s_0_20 <=s (const 64 (2**20)),
(const 64 0) - r_0_20 - s_0_20 <=s (const 64 (2**20))
}




mov x11 u_0_20;
mov x12 v_0_20;
mov x13 r_0_20;
mov x14 s_0_20;
mov x15 u_20_40;
mov x16 v_20_40;
mov x17 r_20_40;
mov x20 s_20_40;



// update_uuvvrrss
(* mul	x9, x15, x11                                #! PC = 0xaaaaca6615e0 *)
smul x9 x15 x11;
(* madd	x10, x16, x13, x9                          #! PC = 0xaaaaca6615e4 *)
smul tmp x13 x16;
sadd x10 x9 tmp;
(* mul	x9, x17, x11                                #! PC = 0xaaaaca6615e8 *)
smul x9 x17 x11;
(* madd	x13, x20, x13, x9                          #! PC = 0xaaaaca6615ec *)
smul tmp x13 x20;
sadd x13 x9 tmp;
(* mov	x11, x10                                    #! PC = 0xaaaaca6615f0 *)
mov x11 x10;
(* mul	x9, x15, x12                                #! PC = 0xaaaaca6615f4 *)
smul x9 x15 x12;
(* madd	x10, x16, x14, x9                          #! PC = 0xaaaaca6615f8 *)
smul tmp x14 x16;
sadd x10 x9 tmp;
(* mul	x9, x17, x12                                #! PC = 0xaaaaca6615fc *)
smul x9 x17 x12;
(* madd	x14, x20, x14, x9                          #! PC = 0xaaaaca661600 *)
smul tmp x14 x20;
sadd x14 x9 tmp;
(* mov	x12, x10                                    #! PC = 0xaaaaca661604 *)
mov x12 x10;

mov u_0_40 x11;
mov v_0_40 x12;
mov r_0_40 x13;
mov s_0_40 x14;


assert
    u_20_40 * u_0_20 + v_20_40 * r_0_20 = u_0_40,
    u_20_40 * v_0_20 + v_20_40 * s_0_20 = v_0_40,
    r_20_40 * u_0_20 + s_20_40 * r_0_20 = r_0_40,
    r_20_40 * v_0_20 + s_20_40 * s_0_20 = s_0_40
    && true
;

assert true &&
    u_20_40 * u_0_20 + v_20_40 * r_0_20 = u_0_40,
    u_20_40 * v_0_20 + v_20_40 * s_0_20 = v_0_40,
    r_20_40 * u_0_20 + s_20_40 * r_0_20 = r_0_40,
    r_20_40 * v_0_20 + s_20_40 * s_0_20 = s_0_40
;

// By theorem from matrix norm, we have
assume true &&
u_0_40 + v_0_40 <=s (const 64 (2**40)),
u_0_40 - v_0_40 <=s (const 64 (2**40)),
(const 64 0) - u_0_40 + v_0_40 <=s (const 64 (2**40)),
(const 64 0) - u_0_40 - v_0_40 <=s (const 64 (2**40)),
r_0_40 + s_0_40 <=s (const 64 (2**40)),
r_0_40 - s_0_40 <=s (const 64 (2**40)),
(const 64 0) - r_0_40 + s_0_40 <=s (const 64 (2**40)),
(const 64 0) - r_0_40 - s_0_40 <=s (const 64 (2**40)),
(const 64 (-(2**40))) <=s u_0_40, u_0_40 <=s (const 64 ((2**40) - 1)),
(const 64 (-(2**40))) <=s v_0_40, v_0_40 <=s (const 64 ((2**40) - 1)),
(const 64 (-(2**40))) <=s r_0_40, r_0_40 <=s (const 64 ((2**40) - 1)),
(const 64 (-(2**40))) <=s s_0_40, s_0_40 <=s (const 64 ((2**40) - 1));

{
u_20_40 * u_0_20 + v_20_40 * r_0_20 = u_0_40,
u_20_40 * v_0_20 + v_20_40 * s_0_20 = v_0_40,
r_20_40 * u_0_20 + s_20_40 * r_0_20 = r_0_40,
r_20_40 * v_0_20 + s_20_40 * s_0_20 = s_0_40
    &&
u_20_40 * u_0_20 + v_20_40 * r_0_20 = u_0_40,
u_20_40 * v_0_20 + v_20_40 * s_0_20 = v_0_40,
r_20_40 * u_0_20 + s_20_40 * r_0_20 = r_0_40,
r_20_40 * v_0_20 + s_20_40 * s_0_20 = s_0_40,
u_0_40 + v_0_40 <=s (const 64 (2**40)),
u_0_40 - v_0_40 <=s (const 64 (2**40)),
(const 64 0) - u_0_40 + v_0_40 <=s (const 64 (2**40)),
(const 64 0) - u_0_40 - v_0_40 <=s (const 64 (2**40)),
r_0_40 + s_0_40 <=s (const 64 (2**40)),
r_0_40 - s_0_40 <=s (const 64 (2**40)),
(const 64 0) - r_0_40 + s_0_40 <=s (const 64 (2**40)),
(const 64 0) - r_0_40 - s_0_40 <=s (const 64 (2**40)),
(const 64 (-(2**40))) <=s u_0_40, u_0_40 <=s (const 64 ((2**40) - 1)),
(const 64 (-(2**40))) <=s v_0_40, v_0_40 <=s (const 64 ((2**40) - 1)),
(const 64 (-(2**40))) <=s r_0_40, r_0_40 <=s (const 64 ((2**40) - 1)),
(const 64 (-(2**40))) <=s s_0_40, s_0_40 <=s (const 64 ((2**40) - 1))
}


proc update_uuvvrrss_2 (
    u_0_40@sint64,
    v_0_40@sint64,
    r_0_40@sint64,
    s_0_40@sint64,
    u_40_59@sint64,
    v_40_59@sint64,
    r_40_59@sint64,
    s_40_59@sint64;
    u_0_59@sint64,
    v_0_59@sint64,
    r_0_59@sint64,
    s_0_59@sint64
) = 
{
  true
        &&

    (const 64 (-(2**40))) <=s u_0_40, u_0_40 <=s (const 64 ((2**40) - 1)),
    (const 64 (-(2**40))) <=s v_0_40, v_0_40 <=s (const 64 ((2**40) - 1)),
    (const 64 (-(2**40))) <=s r_0_40, r_0_40 <=s (const 64 ((2**40) - 1)),
    (const 64 (-(2**40))) <=s s_0_40, s_0_40 <=s (const 64 ((2**40) - 1)),

    (const 64 (-(2**20))) <=s u_40_59, u_40_59 <=s (const 64 ((2**20) - 1)),
    (const 64 (-(2**20))) <=s v_40_59, v_40_59 <=s (const 64 ((2**20) - 1)),
    (const 64 (-(2**20))) <=s r_40_59, r_40_59 <=s (const 64 ((2**20) - 1)),
    (const 64 (-(2**20))) <=s s_40_59, s_40_59 <=s (const 64 ((2**20) - 1)),
    u_40_59 + v_40_59 <=s (const 64 (2**20)),
    u_40_59 - v_40_59 <=s (const 64 (2**20)),
    (const 64 0) - u_40_59 + v_40_59 <=s (const 64 (2**20)),
    (const 64 0) - u_40_59 - v_40_59 <=s (const 64 (2**20)),
    r_40_59 + s_40_59 <=s (const 64 (2**20)),
    r_40_59 - s_40_59 <=s (const 64 (2**20)),
    (const 64 0) - r_40_59 + s_40_59 <=s (const 64 (2**20)),
    (const 64 0) - r_40_59 - s_40_59 <=s (const 64 (2**20)),
    u_0_40 + v_0_40 <=s (const 64 (2**40)),
    u_0_40 - v_0_40 <=s (const 64 (2**40)),
    (const 64 0) - u_0_40 + v_0_40 <=s (const 64 (2**40)),
    (const 64 0) - u_0_40 - v_0_40 <=s (const 64 (2**40)),
    r_0_40 + s_0_40 <=s (const 64 (2**40)),
    r_0_40 - s_0_40 <=s (const 64 (2**40)),
    (const 64 0) - r_0_40 + s_0_40 <=s (const 64 (2**40)),
    (const 64 0) - r_0_40 - s_0_40 <=s (const 64 (2**40))

}




mov x11 u_0_40;
mov x12 v_0_40;
mov x13 r_0_40;
mov x14 s_0_40;
mov x15 u_40_59;
mov x16 v_40_59;
mov x17 r_40_59;
mov x20 s_40_59;



// update_uuvvrrss
(* mul	x9, x15, x11                                #! PC = 0xaaaaca6615e0 *)
smul x9 x15 x11;
(* madd	x10, x16, x13, x9                          #! PC = 0xaaaaca6615e4 *)
smul tmp x13 x16;
sadd x10 x9 tmp;
(* mul	x9, x17, x11                                #! PC = 0xaaaaca6615e8 *)
smul x9 x17 x11;
(* madd	x13, x20, x13, x9                          #! PC = 0xaaaaca6615ec *)
smul tmp x13 x20;
sadd x13 x9 tmp;
(* mov	x11, x10                                    #! PC = 0xaaaaca6615f0 *)
mov x11 x10;
(* mul	x9, x15, x12                                #! PC = 0xaaaaca6615f4 *)
smul x9 x15 x12;
(* madd	x10, x16, x14, x9                          #! PC = 0xaaaaca6615f8 *)
smul tmp x14 x16;
sadd x10 x9 tmp;
(* mul	x9, x17, x12                                #! PC = 0xaaaaca6615fc *)
smul x9 x17 x12;
(* madd	x14, x20, x14, x9                          #! PC = 0xaaaaca661600 *)
smul tmp x14 x20;
sadd x14 x9 tmp;
(* mov	x12, x10                                    #! PC = 0xaaaaca661604 *)
mov x12 x10;

mov u_0_59 x11;
mov v_0_59 x12;
mov r_0_59 x13;
mov s_0_59 x14;


assert
    u_40_59 * u_0_40 + v_40_59 * r_0_40 = u_0_59,
    u_40_59 * v_0_40 + v_40_59 * s_0_40 = v_0_59,
    r_40_59 * u_0_40 + s_40_59 * r_0_40 = r_0_59,
    r_40_59 * v_0_40 + s_40_59 * s_0_40 = s_0_59
    && true
;

assert true &&
    u_40_59 * u_0_40 + v_40_59 * r_0_40 = u_0_59,
    u_40_59 * v_0_40 + v_40_59 * s_0_40 = v_0_59,
    r_40_59 * u_0_40 + s_40_59 * r_0_40 = r_0_59,
    r_40_59 * v_0_40 + s_40_59 * s_0_40 = s_0_59
;

// By theorem from matrix norm, we have
assume true &&
u_0_59 + v_0_59 <=s (const 64 (2**60)),
u_0_59 - v_0_59 <=s (const 64 (2**60)),
(const 64 0) - u_0_59 + v_0_59 <=s (const 64 (2**60)),
(const 64 0) - u_0_59 - v_0_59 <=s (const 64 (2**60)),
r_0_59 + s_0_59 <=s (const 64 (2**60)),
r_0_59 - s_0_59 <=s (const 64 (2**60)),
(const 64 0) - r_0_59 + s_0_59 <=s (const 64 (2**60)),
(const 64 0) - r_0_59 - s_0_59 <=s (const 64 (2**60)),
(const 64 (-(2**60))) <=s u_0_59, u_0_59 <=s (const 64 ((2**60) - 1)),
(const 64 (-(2**60))) <=s v_0_59, v_0_59 <=s (const 64 ((2**60) - 1)),
(const 64 (-(2**60))) <=s r_0_59, r_0_59 <=s (const 64 ((2**60) - 1)),
(const 64 (-(2**60))) <=s s_0_59, s_0_59 <=s (const 64 ((2**60) - 1));

{
    u_40_59 * u_0_40 + v_40_59 * r_0_40 = u_0_59,
    u_40_59 * v_0_40 + v_40_59 * s_0_40 = v_0_59,
    r_40_59 * u_0_40 + s_40_59 * r_0_40 = r_0_59,
    r_40_59 * v_0_40 + s_40_59 * s_0_40 = s_0_59
    &&
    u_40_59 * u_0_40 + v_40_59 * r_0_40 = u_0_59,
    u_40_59 * v_0_40 + v_40_59 * s_0_40 = v_0_59,
    r_40_59 * u_0_40 + s_40_59 * r_0_40 = r_0_59,
    r_40_59 * v_0_40 + s_40_59 * s_0_40 = s_0_59,
u_0_59 + v_0_59 <=s (const 64 (2**60)),
u_0_59 - v_0_59 <=s (const 64 (2**60)),
(const 64 0) - u_0_59 + v_0_59 <=s (const 64 (2**60)),
(const 64 0) - u_0_59 - v_0_59 <=s (const 64 (2**60)),
r_0_59 + s_0_59 <=s (const 64 (2**60)),
r_0_59 - s_0_59 <=s (const 64 (2**60)),
(const 64 0) - r_0_59 + s_0_59 <=s (const 64 (2**60)),
(const 64 0) - r_0_59 - s_0_59 <=s (const 64 (2**60)),
(const 64 (-(2**60))) <=s u_0_59, u_0_59 <=s (const 64 ((2**60) - 1)),
(const 64 (-(2**60))) <=s v_0_59, v_0_59 <=s (const 64 ((2**60) - 1)),
(const 64 (-(2**60))) <=s r_0_59, r_0_59 <=s (const 64 ((2**60) - 1)),
(const 64 (-(2**60))) <=s s_0_59, s_0_59 <=s (const 64 ((2**60) - 1))
}

proc divstepx20_1(
    f_low60_0@sint64,
    g_low60_0@sint64,
    const_2p20a2p41@sint64,
    delta@sint64, delta_range_in@sint64
    ;
    neg_f_low60_20@sint64,
    neg_g_low60_20@sint64,
    u@sint64,
    v@sint64,
    r@sint64,
    s@sint64,
    delta_out@sint64, delta_range_out@sint64
) = {
true 
&&
f_low60_0 = 1@sint64 (mod (2@sint64)),
const_2p20a2p41 = (2**20 + 2**41)@sint64,

((const 64 0)-delta_range_in) <=s delta, delta <=s delta_range_in,
delta_range_in >=s (const 64 0),
delta_range_in <=s 1300@sint64
}


mov x1 f_low60_0;
mov x2 g_low60_0;
mov x3 delta;
mov x6 const_2p20a2p41;

// init_fuv_grs
(* and	x7, x1, #0xfffff                            #! PC = 0xaaaabe370fe0 *)
and x7@uint64 x1 0xfffff@uint64;
(* and	x8, x2, #0xfffff                            #! PC = 0xaaaabe370fe4 *)
and x8@uint64 x2 0xfffff@uint64;
(* orr	x7, x7, #0xfffffe0000000000                 #! PC = 0xaaaabe370fe8 *)
or x7@uint64 x7 0xfffffe0000000000@uint64;
(* orr	x8, x8, #0xc000000000000000                 #! PC = 0xaaaabe370fec *)
or x8@uint64 x8 0xc000000000000000@uint64;

cast x1@sint64 x1;
cast x2@sint64 x2;
cast x7@sint64 x7;
cast x8@sint64 x8;

mov fuv x7;
mov grs x8;
and f_low60_0_low20_0@sint64 f_low60_0 (2**20 - 1)@sint64;
and g_low60_0_low20_0@sint64 g_low60_0 (2**20 - 1)@sint64;
mov u_init (-(2**20))@sint64;
mov v_init ( 0)@sint64;
mov r_init ( 0)@sint64;
mov s_init (-(2**20))@sint64;


assert 
u_init * f_low60_0_low20_0 + v_init * g_low60_0_low20_0 = f_low60_0_low20_0 * (-(2**20)),
r_init * f_low60_0_low20_0 + s_init * g_low60_0_low20_0 = g_low60_0_low20_0 * (-(2**20))
&&
fuv = f_low60_0_low20_0 + u_init * (const 64 (2**21)) + v_init * (const 64 (2**42)),
grs = g_low60_0_low20_0 + r_init * (const 64 (2**21)) + s_init * (const 64 (2**42)),
(const 64 (-(2**20)+1)) <=s f_low60_0_low20_0,
f_low60_0_low20_0 <=s (const 64 ((2**20)-1)),
(const 64 (-(2**20)+1)) <=s g_low60_0_low20_0,
g_low60_0_low20_0 <=s (const 64 ((2**20)-1)),
fuv = (const 64 1) (mod (const 64 2)),
u_init = (const 64 (-(2**20))),
v_init = (const 64 (0)),
r_init = (const 64 (0)),
s_init = (const 64 (-(2**20)))
;

// divstepx20

mov delta_range delta_range_in;

// 1
call divstep_ver_a(x7, x8, x3, delta_range; x7, x8 ,x3, ne, delta_range);
// 2
call divstep_ver_b(x7, x8, x3, ne, delta_range; x7, x8 ,x3, ne, delta_range);
// 3
call divstep_ver_b(x7, x8, x3, ne, delta_range; x7, x8 ,x3, ne, delta_range);
// 4
call divstep_ver_b(x7, x8, x3, ne, delta_range; x7, x8 ,x3, ne, delta_range);
// 5
call divstep_ver_b(x7, x8, x3, ne, delta_range; x7, x8 ,x3, ne, delta_range);
// 6
call divstep_ver_b(x7, x8, x3, ne, delta_range; x7, x8 ,x3, ne, delta_range);
// 7
call divstep_ver_b(x7, x8, x3, ne, delta_range; x7, x8 ,x3, ne, delta_range);
// 8
call divstep_ver_b(x7, x8, x3, ne, delta_range; x7, x8 ,x3, ne, delta_range);
// 9
call divstep_ver_b(x7, x8, x3, ne, delta_range; x7, x8 ,x3, ne, delta_range);
// 10
call divstep_ver_b(x7, x8, x3, ne, delta_range; x7, x8 ,x3, ne, delta_range);
// 11
call divstep_ver_b(x7, x8, x3, ne, delta_range; x7, x8 ,x3, ne, delta_range);
// 12
call divstep_ver_b(x7, x8, x3, ne, delta_range; x7, x8 ,x3, ne, delta_range);
// 13
call divstep_ver_b(x7, x8, x3, ne, delta_range; x7, x8 ,x3, ne, delta_range);
// 14
call divstep_ver_b(x7, x8, x3, ne, delta_range; x7, x8 ,x3, ne, delta_range);
// 15
call divstep_ver_b(x7, x8, x3, ne, delta_range; x7, x8 ,x3, ne, delta_range);
// 16
call divstep_ver_b(x7, x8, x3, ne, delta_range; x7, x8 ,x3, ne, delta_range);
// 17
call divstep_ver_b(x7, x8, x3, ne, delta_range; x7, x8 ,x3, ne, delta_range);
// 18
call divstep_ver_b(x7, x8, x3, ne, delta_range; x7, x8 ,x3, ne, delta_range);
// 19
call divstep_ver_b(x7, x8, x3, ne, delta_range; x7, x8 ,x3, ne, delta_range);
// 20
call divstep_ver_c(x7, x8, x3, ne, delta_range; x7, x8 ,x3, delta_range);

mov delta_range_out delta_range;
mov delta_out x3;


// from lemma we have
mov fuv x7;
mov grs x8;
nondet u_done@sint64;
nondet v_done@sint64;
nondet r_done@sint64;
nondet s_done@sint64;
nondet f_low60_0_low20_20@sint64;
nondet g_low60_0_low20_20@sint64;

assume 
u_done * f_low60_0_low20_0 + v_done * g_low60_0_low20_0 = f_low60_0_low20_20 * (-(2**20)),
r_done * f_low60_0_low20_0 + s_done * g_low60_0_low20_0 = g_low60_0_low20_20 * (-(2**20))
&&
fuv = f_low60_0_low20_20 + u_done * (const 64 (2**21)) + v_done * (const 64 (2**42)),
grs = g_low60_0_low20_20 + r_done * (const 64 (2**21)) + s_done * (const 64 (2**42)),
(const 64 (-(2**20)+1)) <=s f_low60_0_low20_20,
f_low60_0_low20_20 <=s (const 64 ((2**20)-1)),
(const 64 (-(2**20)+1)) <=s g_low60_0_low20_20,
g_low60_0_low20_20 <=s (const 64 ((2**20)-1)),
(const 64 (-(2**20))) <=s u_done, u_done <=s (const 64 ((2**20)-1)),
(const 64 (-(2**20))) <=s v_done, v_done <=s (const 64 ((2**20)-1)),
(const 64 (-(2**20))) <=s r_done, r_done <=s (const 64 ((2**20)-1)),
(const 64 (-(2**20))) <=s s_done, s_done <=s (const 64 ((2**20)-1)),
u_done + v_done <=s (const 64 (2**20)),
u_done - v_done <=s (const 64 (2**20)),
(const 64 0) - u_done + v_done <=s (const 64 (2**20)),
(const 64 0) - u_done - v_done <=s (const 64 (2**20)),
r_done + s_done <=s (const 64 (2**20)),
r_done - s_done <=s (const 64 (2**20)),
(const 64 0) - r_done + s_done <=s (const 64 (2**20)),
(const 64 0) - r_done - s_done <=s (const 64 (2**20))
;

// by the theory of jumpdivstep, we have
nondet f_low60_20@sint64;
nondet g_low60_20@sint64;

assume
u_done * f_low60_0 + v_done * g_low60_0 = f_low60_20 * (-(2**20)),
r_done * f_low60_0 + s_done * g_low60_0 = g_low60_20 * (-(2**20))
&&
f_low60_20 = (const 64 1) (mod (const 64 2))
;


// extraction
(* add	x12, x7, x6                                 #! PC = 0xaaaaafca0c08 *)
add x12 x7 x6;
(* asr	x12, x12, #42                               #! PC = 0xaaaaafca0c0c *)
cast x12@sint64 x12;
split x12 dc x12 42;
(* add	x11, x7, #0x100, lsl #12                    #! PC = 0xaaaaafca0c10 *)
add x11 x7 (2**20)@sint64;
(* lsl	x11, x11, #22                               #! PC = 0xaaaaafca0c14 *)
split dcH x11 x11 (64-22); shl x11 x11 22;
(* asr	x11, x11, #43                               #! PC = 0xaaaaafca0c18 *)
cast x11@sint64 x11;
split x11 dc x11 43;
(* add	x14, x8, x6                                 #! PC = 0xaaaaafca0c1c *)
add x14 x8 x6;
(* asr	x14, x14, #42                               #! PC = 0xaaaaafca0c20 *)
cast x14@sint64 x14;
split x14 dc x14 42;
(* add	x13, x8, #0x100, lsl #12                    #! PC = 0xaaaaafca0c24 *)
add x13 x8 (2**20)@sint64;
(* lsl	x13, x13, #22                               #! PC = 0xaaaaafca0c28 *)
split dcH x13 x13 (64-22); shl x13 x13 22;
(* asr	x13, x13, #43                               #! PC = 0xaaaaafca0c2c *)
cast x13@sint64 x13;
split x13 dc x13 43;

mov u x11;
mov v x12;
mov r x13;
mov s x14;
assert true && u = u_done;
assert true && v = v_done;
assert true && r = r_done;
assert true && s = s_done;
assume u = u_done && u = u_done;
assume v = v_done && v = v_done;
assume r = r_done && r = r_done;
assume s = s_done && s = s_done;




// update_fg
(* mul	x9, x11, x1                                 #! PC = 0xaaaaafca0c30 *)
mull dcH x9 x11 x1;
(* madd	x9, x12, x2, x9                            #! PC = 0xaaaaafca0c34 *)
mull dcH tmp x2 x12;
adds dc x9 x9 tmp;

assert 
x9 = 0 (mod (2**20)),
x9 = (-1) * ((2**20)) * f_low60_20 (mod (2**64))
&& true;
assume 
x9 = 0 (mod (2**20)),
x9 = (-1) * ((2**20)) * f_low60_20 (mod (2**64))
&&
x9 = (const 64 0) (mod (const 64 (2**20))),
x9 = (const 64 (-1)) * (const 64 (2**20)) * f_low60_20;

(* asr	x9, x9, #20                                 #! PC = 0xaaaaafca0c38 *)
split x9 dc x9 20;
(* mul	x10, x13, x1                                #! PC = 0xaaaaafca0c3c *)
mull dcH x10 x13 x1;
(* madd	x10, x14, x2, x10                          #! PC = 0xaaaaafca0c40 *)
mull dcH tmp x2 x14;
adds dc x10 x10 tmp;

assert 
x10 = 0 (mod (2**20)),
x10 = (-1) * ((2**20)) * g_low60_20 (mod (2**64))
&& true;
assume 
x10 = 0 (mod (2**20)),
x10 = (-1) * ((2**20)) * g_low60_20 (mod (2**64))
&&
x10 = (const 64 0) (mod (const 64 (2**20))),
x10 = (const 64 (-1)) * (const 64 (2**20)) * g_low60_20;

(* asr	x2, x10, #20                                #! PC = 0xaaaaafca0c44 *)
split x2 dc x10 20;
(* mov	x1, x9                                      #! PC = 0xaaaaafca0c48 *)
mov x1 x9;



assert true && x1 = (const 64 (-1)) * f_low60_20 (mod (const 64 (2**44)));
assert true && x2 = (const 64 (-1)) * g_low60_20 (mod (const 64 (2**44)));

assume 
x1 = -f_low60_20 (mod (2**44)),
x2 = -g_low60_20 (mod (2**44))
&&
x1 = (const 64 (-1)) * f_low60_20 (mod (const 64 (2**44))),
x2 = (const 64 (-1)) * g_low60_20 (mod (const 64 (2**44)));


mov neg_f_low60_20 x1;
mov neg_g_low60_20 x2;

assert 
u_done * f_low60_0 + v_done * g_low60_0 = neg_f_low60_20 * (2**20) (mod (2**64)),
r_done * f_low60_0 + s_done * g_low60_0 = neg_g_low60_20 * (2**20) (mod (2**64))
&& true;



{
u * f_low60_0 + v * g_low60_0 = neg_f_low60_20 * (2**20) (mod (2**64)),
r * f_low60_0 + s * g_low60_0 = neg_g_low60_20 * (2**20) (mod (2**64))
&&
neg_f_low60_20 = (1@sint64) (mod (2@sint64)),
(const 64 (-(2**20))) <=s u, u <=s (const 64 ((2**20)-1)),
(const 64 (-(2**20))) <=s v, v <=s (const 64 ((2**20)-1)),
(const 64 (-(2**20))) <=s r, r <=s (const 64 ((2**20)-1)),
(const 64 (-(2**20))) <=s s, s <=s (const 64 ((2**20)-1)),
u + v <=s (const 64 (2**20)),
u - v <=s (const 64 (2**20)),
(const 64 0) - u + v <=s (const 64 (2**20)),
(const 64 0) - u - v <=s (const 64 (2**20)),
r + s <=s (const 64 (2**20)),
r - s <=s (const 64 (2**20)),
(const 64 0) - r + s <=s (const 64 (2**20)),
(const 64 0) - r - s <=s (const 64 (2**20)),
((const 64 0)-delta_range_out) <=s delta_out, delta_out <=s delta_range_out,
delta_range_out = delta_range_in + (const 64 (2*20))
}



proc divstepx20_2(
    neg_f_low60_20@sint64,
    neg_g_low60_20@sint64,
    f_low60_0@sint64,
    g_low60_0@sint64,
    u_in@sint64,
    v_in@sint64,
    r_in@sint64,
    s_in@sint64,
    const_2p20a2p41@sint64,
    delta@sint64, delta_range_in@sint64
    ;
    f_low60_40@sint64,
    g_low60_40@sint64,
    u@sint64,
    v@sint64,
    r@sint64,
    s@sint64,
    delta_out@sint64, delta_range_out@sint64
) = {
u_in * f_low60_0 + v_in * g_low60_0 = neg_f_low60_20 * (2**20) (mod (2**64)),
r_in * f_low60_0 + s_in * g_low60_0 = neg_g_low60_20 * (2**20) (mod (2**64))
&&
neg_f_low60_20 = 1@sint64 (mod (2@sint64)),


(const 64 (-(2**20))) <=s u_in, u_in <=s (const 64 ((2**20)-1)),
(const 64 (-(2**20))) <=s v_in, v_in <=s (const 64 ((2**20)-1)),
(const 64 (-(2**20))) <=s r_in, r_in <=s (const 64 ((2**20)-1)),
(const 64 (-(2**20))) <=s s_in, s_in <=s (const 64 ((2**20)-1)),
u_in + v_in <=s (const 64 (2**20)),
u_in - v_in <=s (const 64 (2**20)),
(const 64 0) - u_in + v_in <=s (const 64 (2**20)),
(const 64 0) - u_in - v_in <=s (const 64 (2**20)),
r_in + s_in <=s (const 64 (2**20)),
r_in - s_in <=s (const 64 (2**20)),
(const 64 0) - r_in + s_in <=s (const 64 (2**20)),
(const 64 0) - r_in - s_in <=s (const 64 (2**20)),


const_2p20a2p41 = (2**20 + 2**41)@sint64,
((const 64 0)-delta_range_in) <=s delta, delta <=s delta_range_in,
delta_range_in >=s (const 64 0),
delta_range_in <=s 1300@sint64
}

mov x1 neg_f_low60_20;
mov x2 neg_g_low60_20;
mov x3 delta;
mov x6 const_2p20a2p41;

// init_fuv_grs
(* and	x7, x1, #0xfffff                            #! PC = 0xaaaabe370fe0 *)
and x7@uint64 x1 0xfffff@uint64;
(* and	x8, x2, #0xfffff                            #! PC = 0xaaaabe370fe4 *)
and x8@uint64 x2 0xfffff@uint64;
(* orr	x7, x7, #0xfffffe0000000000                 #! PC = 0xaaaabe370fe8 *)
or x7@uint64 x7 0xfffffe0000000000@uint64;
(* orr	x8, x8, #0xc000000000000000                 #! PC = 0xaaaabe370fec *)
or x8@uint64 x8 0xc000000000000000@uint64;

cast x1@sint64 x1;
cast x2@sint64 x2;
cast x7@sint64 x7;
cast x8@sint64 x8;

mov fuv x7;
mov grs x8;
and neg_f_low60_20_low20_0@sint64 neg_f_low60_20 (2**20 - 1)@sint64;
and neg_g_low60_20_low20_0@sint64 neg_g_low60_20 (2**20 - 1)@sint64;
mov u_init (-(2**20))@sint64;
mov v_init ( 0)@sint64;
mov r_init ( 0)@sint64;
mov s_init (-(2**20))@sint64;

assert 
u_init * neg_f_low60_20_low20_0 + v_init * neg_g_low60_20_low20_0 = neg_f_low60_20_low20_0 * (-(2**20)),
r_init * neg_f_low60_20_low20_0 + s_init * neg_g_low60_20_low20_0 = neg_g_low60_20_low20_0 * (-(2**20))
&&
fuv = neg_f_low60_20_low20_0 + u_init * (const 64 (2**21)) + v_init * (const 64 (2**42)),
grs = neg_g_low60_20_low20_0 + r_init * (const 64 (2**21)) + s_init * (const 64 (2**42)),
(const 64 (-(2**20)+1)) <=s neg_f_low60_20_low20_0,
neg_f_low60_20_low20_0 <=s (const 64 ((2**20)-1)),
(const 64 (-(2**20)+1)) <=s neg_g_low60_20_low20_0,
neg_g_low60_20_low20_0 <=s (const 64 ((2**20)-1)),
fuv = (const 64 1) (mod (const 64 2)),
u_init = (const 64 (-(2**20))),
v_init = (const 64 (0)),
r_init = (const 64 (0)),
s_init = (const 64 (-(2**20)))
;
// divstepx20

mov delta_range delta_range_in;

// 1
call divstep_ver_a(x7, x8, x3, delta_range; x7, x8 ,x3, ne, delta_range);
// 2
call divstep_ver_b(x7, x8, x3, ne, delta_range; x7, x8 ,x3, ne, delta_range);
// 3
call divstep_ver_b(x7, x8, x3, ne, delta_range; x7, x8 ,x3, ne, delta_range);
// 4
call divstep_ver_b(x7, x8, x3, ne, delta_range; x7, x8 ,x3, ne, delta_range);
// 5
call divstep_ver_b(x7, x8, x3, ne, delta_range; x7, x8 ,x3, ne, delta_range);
// 6
call divstep_ver_b(x7, x8, x3, ne, delta_range; x7, x8 ,x3, ne, delta_range);
// 7
call divstep_ver_b(x7, x8, x3, ne, delta_range; x7, x8 ,x3, ne, delta_range);
// 8
call divstep_ver_b(x7, x8, x3, ne, delta_range; x7, x8 ,x3, ne, delta_range);
// 9
call divstep_ver_b(x7, x8, x3, ne, delta_range; x7, x8 ,x3, ne, delta_range);
// 10
call divstep_ver_b(x7, x8, x3, ne, delta_range; x7, x8 ,x3, ne, delta_range);
// 11
call divstep_ver_b(x7, x8, x3, ne, delta_range; x7, x8 ,x3, ne, delta_range);
// 12
call divstep_ver_b(x7, x8, x3, ne, delta_range; x7, x8 ,x3, ne, delta_range);
// 13
call divstep_ver_b(x7, x8, x3, ne, delta_range; x7, x8 ,x3, ne, delta_range);
// 14
call divstep_ver_b(x7, x8, x3, ne, delta_range; x7, x8 ,x3, ne, delta_range);
// 15
call divstep_ver_b(x7, x8, x3, ne, delta_range; x7, x8 ,x3, ne, delta_range);
// 16
call divstep_ver_b(x7, x8, x3, ne, delta_range; x7, x8 ,x3, ne, delta_range);
// 17
call divstep_ver_b(x7, x8, x3, ne, delta_range; x7, x8 ,x3, ne, delta_range);
// 18
call divstep_ver_b(x7, x8, x3, ne, delta_range; x7, x8 ,x3, ne, delta_range);
// 19
call divstep_ver_b(x7, x8, x3, ne, delta_range; x7, x8 ,x3, ne, delta_range);
// 20
call divstep_ver_c(x7, x8, x3, ne, delta_range; x7, x8 ,x3, delta_range);

mov delta_range_out delta_range;
mov delta_out x3;



// from lemma we have
mov fuv x7;
mov grs x8;
nondet u_done@sint64;
nondet v_done@sint64;
nondet r_done@sint64;
nondet s_done@sint64;
nondet neg_f_low60_20_low20_20@sint64;
nondet neg_g_low60_20_low20_20@sint64;

assume 
u_done * neg_f_low60_20_low20_0 + v_done * neg_g_low60_20_low20_0 = neg_f_low60_20_low20_20 * (-(2**20)),
r_done * neg_f_low60_20_low20_0 + s_done * neg_g_low60_20_low20_0 = neg_g_low60_20_low20_20 * (-(2**20))
&&
fuv = neg_f_low60_20_low20_20 + u_done * (const 64 (2**21)) + v_done * (const 64 (2**42)),
grs = neg_g_low60_20_low20_20 + r_done * (const 64 (2**21)) + s_done * (const 64 (2**42)),
(const 64 (-(2**20)+1)) <=s neg_f_low60_20_low20_20,
neg_f_low60_20_low20_20 <=s (const 64 ((2**20)-1)),
(const 64 (-(2**20)+1)) <=s neg_g_low60_20_low20_20,
neg_g_low60_20_low20_20 <=s (const 64 ((2**20)-1)),
(const 64 (-(2**20))) <=s u_done, u_done <=s (const 64 ((2**20)-1)),
(const 64 (-(2**20))) <=s v_done, v_done <=s (const 64 ((2**20)-1)),
(const 64 (-(2**20))) <=s r_done, r_done <=s (const 64 ((2**20)-1)),
(const 64 (-(2**20))) <=s s_done, s_done <=s (const 64 ((2**20)-1)),
u_done + v_done <=s (const 64 (2**20)),
u_done - v_done <=s (const 64 (2**20)),
(const 64 0) - u_done + v_done <=s (const 64 (2**20)),
(const 64 0) - u_done - v_done <=s (const 64 (2**20)),
r_done + s_done <=s (const 64 (2**20)),
r_done - s_done <=s (const 64 (2**20)),
(const 64 0) - r_done + s_done <=s (const 64 (2**20)),
(const 64 0) - r_done - s_done <=s (const 64 (2**20))
;

// by the theory of jumpdivstep, we have
nondet neg_f_low60_40@sint64;
nondet neg_g_low60_40@sint64;

assume
u_done * neg_f_low60_20 + v_done * neg_g_low60_20 = neg_f_low60_40 * (-(2**20)),
r_done * neg_f_low60_20 + s_done * neg_g_low60_20 = neg_g_low60_40 * (-(2**20))
&&
neg_f_low60_40 = (const 64 1) (mod (const 64 2))
;


// extraction
(* add	x12, x7, x6                                 #! PC = 0xaaaaafca0c08 *)
add x12 x7 x6;
(* asr	x12, x12, #42                               #! PC = 0xaaaaafca0c0c *)
cast x12@sint64 x12;
split x12 dc x12 42;
(* add	x11, x7, #0x100, lsl #12                    #! PC = 0xaaaaafca0c10 *)
add x11 x7 (2**20)@sint64;
(* lsl	x11, x11, #22                               #! PC = 0xaaaaafca0c14 *)
split dcH x11 x11 (64-22); shl x11 x11 22;
(* asr	x11, x11, #43                               #! PC = 0xaaaaafca0c18 *)
cast x11@sint64 x11;
split x11 dc x11 43;
(* add	x14, x8, x6                                 #! PC = 0xaaaaafca0c1c *)
add x14 x8 x6;
(* asr	x14, x14, #42                               #! PC = 0xaaaaafca0c20 *)
cast x14@sint64 x14;
split x14 dc x14 42;
(* add	x13, x8, #0x100, lsl #12                    #! PC = 0xaaaaafca0c24 *)
add x13 x8 (2**20)@sint64;
(* lsl	x13, x13, #22                               #! PC = 0xaaaaafca0c28 *)
split dcH x13 x13 (64-22); shl x13 x13 22;
(* asr	x13, x13, #43                               #! PC = 0xaaaaafca0c2c *)
cast x13@sint64 x13;
split x13 dc x13 43;

mov u x11;
mov v x12;
mov r x13;
mov s x14;
assert true && u = u_done;
assert true && v = v_done;
assert true && r = r_done;
assert true && s = s_done;
assume u = u_done && u = u_done;
assume v = v_done && v = v_done;
assume r = r_done && r = r_done;
assume s = s_done && s = s_done;

// update_fg
(* mul	x9, x11, x1                                 #! PC = 0xaaaaafca0c30 *)
mull dcH x9 x11 x1;
(* madd	x9, x12, x2, x9                            #! PC = 0xaaaaafca0c34 *)
mull dcH tmp x2 x12;
adds dc x9 x9 tmp;

assert 
x9 = 0 (mod (2**20)),
x9 = (-1) * ((2**20)) * neg_f_low60_40 (mod (2**64))
&& true;
assume 
x9 = 0 (mod (2**20)),
x9 = (-1) * ((2**20)) * neg_f_low60_40 (mod (2**64))
&&
x9 = (const 64 0) (mod (const 64 (2**20))),
x9 = (const 64 (-1)) * (const 64 (2**20)) * neg_f_low60_40;

(* asr	x9, x9, #20                                 #! PC = 0xaaaaafca0c38 *)
split x9 dc x9 20;
(* mul	x10, x13, x1                                #! PC = 0xaaaaafca0c3c *)
mull dcH x10 x13 x1;
(* madd	x10, x14, x2, x10                          #! PC = 0xaaaaafca0c40 *)
mull dcH tmp x2 x14;
adds dc x10 x10 tmp;

assert 
x10 = 0 (mod (2**20)),
x10 = (-1) * ((2**20)) * neg_g_low60_40 (mod (2**64))
&& true;
assume 
x10 = 0 (mod (2**20)),
x10 = (-1) * ((2**20)) * neg_g_low60_40 (mod (2**64))
&&
x10 = (const 64 0) (mod (const 64 (2**20))),
x10 = (const 64 (-1)) * (const 64 (2**20)) * neg_g_low60_40;

(* asr	x2, x10, #20                                #! PC = 0xaaaaafca0c44 *)
split x2 dc x10 20;
(* mov	x1, x9                                      #! PC = 0xaaaaafca0c48 *)
mov x1 x9;



assert
true
&&
x1 = (const 64 (-1)) * neg_f_low60_40 (mod (const 64 (2**44))),
x2 = (const 64 (-1)) * neg_g_low60_40 (mod (const 64 (2**44)));
assume
x1 = -neg_f_low60_40 (mod (2**44)),
x2 = -neg_g_low60_40 (mod (2**44))
&&
x1 = (const 64 (-1)) * neg_f_low60_40 (mod (const 64 (2**44))),
x2 = (const 64 (-1)) * neg_g_low60_40 (mod (const 64 (2**44)));

mov f_low60_40 x1;
mov g_low60_40 x2;
// update_uuvvrrss

call update_uuvvrrss(
    u_in, v_in, r_in, s_in,
    u, v, r, s;
    u, v, r, s
);


assert 
u * f_low60_0 + v * g_low60_0 = f_low60_40 * (2**40) (mod (2**64)),
r * f_low60_0 + s * g_low60_0 = g_low60_40 * (2**40) (mod (2**64))
&& true;


{
u * f_low60_0 + v * g_low60_0 = f_low60_40 * (2**40) (mod (2**64)),
r * f_low60_0 + s * g_low60_0 = g_low60_40 * (2**40) (mod (2**64))
&&
f_low60_40 = (1@sint64) (mod (2@sint64)),
(const 64 (-(2**40))) <=s u, u <=s (const 64 ((2**40)-1)),
(const 64 (-(2**40))) <=s v, v <=s (const 64 ((2**40)-1)),
(const 64 (-(2**40))) <=s r, r <=s (const 64 ((2**40)-1)),
(const 64 (-(2**40))) <=s s, s <=s (const 64 ((2**40)-1)),
u + v <=s (const 64 (2**40)),
u - v <=s (const 64 (2**40)),
(const 64 0) - u + v <=s (const 64 (2**40)),
(const 64 0) - u - v <=s (const 64 (2**40)),
r + s <=s (const 64 (2**40)),
r - s <=s (const 64 (2**40)),
(const 64 0) - r + s <=s (const 64 (2**40)),
(const 64 0) - r - s <=s (const 64 (2**40)),

((const 64 0)-delta_range_out) <=s delta_out, delta_out <=s delta_range_out,
delta_range_out = delta_range_in + (const 64 (2*20))
}


proc divstepx19_3(
    f_low60_40@sint64,
    g_low60_40@sint64,
    f_low60_0@sint64,
    g_low60_0@sint64,
    u_in@sint64,
    v_in@sint64,
    r_in@sint64,
    s_in@sint64,
    const_2p20a2p41@sint64,
    delta@sint64, delta_range_in@sint64
    ;
    neg_f_low60_59@sint64,
    neg_g_low60_59@sint64,
    u@sint64,
    v@sint64,
    r@sint64,
    s@sint64,
    delta_out@sint64, delta_range_out@sint64
) = {
u_in * f_low60_0 + v_in * g_low60_0 = f_low60_40 * (2**40) (mod (2**64)),
r_in * f_low60_0 + s_in * g_low60_0 = g_low60_40 * (2**40) (mod (2**64))
&&
f_low60_40 = 1@sint64 (mod (2@sint64)),


(const 64 (-(2**40))) <=s u_in, u_in <=s (const 64 ((2**40)-1)),
(const 64 (-(2**40))) <=s v_in, v_in <=s (const 64 ((2**40)-1)),
(const 64 (-(2**40))) <=s r_in, r_in <=s (const 64 ((2**40)-1)),
(const 64 (-(2**40))) <=s s_in, s_in <=s (const 64 ((2**40)-1)),
u_in + v_in <=s (const 64 (2**40)),
u_in - v_in <=s (const 64 (2**40)),
(const 64 0) - u_in + v_in <=s (const 64 (2**40)),
(const 64 0) - u_in - v_in <=s (const 64 (2**40)),
r_in + s_in <=s (const 64 (2**40)),
r_in - s_in <=s (const 64 (2**40)),
(const 64 0) - r_in + s_in <=s (const 64 (2**40)),
(const 64 0) - r_in - s_in <=s (const 64 (2**40)),


const_2p20a2p41 = (2**20 + 2**41)@sint64,
((const 64 0)-delta_range_in) <=s delta, delta <=s delta_range_in,
delta_range_in >=s (const 64 0),
delta_range_in <=s 1300@sint64
}


mov x1 f_low60_40;
mov x2 g_low60_40;
mov x3 delta;
mov x6 const_2p20a2p41;



// init_fuv_grs
(* and	x7, x1, #0xfffff                            #! PC = 0xaaaabe370fe0 *)
and x7@uint64 x1 0xfffff@uint64;
(* and	x8, x2, #0xfffff                            #! PC = 0xaaaabe370fe4 *)
and x8@uint64 x2 0xfffff@uint64;
(* orr	x7, x7, #0xfffffe0000000000                 #! PC = 0xaaaabe370fe8 *)
or x7@uint64 x7 0xfffffe0000000000@uint64;
(* orr	x8, x8, #0xc000000000000000                 #! PC = 0xaaaabe370fec *)
or x8@uint64 x8 0xc000000000000000@uint64;

cast x1@sint64 x1;
cast x2@sint64 x2;
cast x7@sint64 x7;
cast x8@sint64 x8;

mov fuv x7;
mov grs x8;
and f_low60_40_low20_0@sint64 f_low60_40 (2**20 - 1)@sint64;
and g_low60_40_low20_0@sint64 g_low60_40 (2**20 - 1)@sint64;
mov u_init (-(2**20))@sint64;
mov v_init ( 0)@sint64;
mov r_init ( 0)@sint64;
mov s_init (-(2**20))@sint64;

assert 
u_init * f_low60_40_low20_0 + v_init * g_low60_40_low20_0 = f_low60_40_low20_0 * (-(2**20)),
r_init * f_low60_40_low20_0 + s_init * g_low60_40_low20_0 = g_low60_40_low20_0 * (-(2**20))
&&
fuv = f_low60_40_low20_0 + u_init * (const 64 (2**21)) + v_init * (const 64 (2**42)),
grs = g_low60_40_low20_0 + r_init * (const 64 (2**21)) + s_init * (const 64 (2**42)),
(const 64 (-(2**20)+1)) <=s f_low60_40_low20_0,
f_low60_40_low20_0 <=s (const 64 ((2**20)-1)),
(const 64 (-(2**20)+1)) <=s g_low60_40_low20_0,
g_low60_40_low20_0 <=s (const 64 ((2**20)-1)),
fuv = (const 64 1) (mod (const 64 2)),
u_init = (const 64 (-(2**20))),
v_init = (const 64 (0)),
r_init = (const 64 (0)),
s_init = (const 64 (-(2**20)))
;

// divstepx19

mov delta_range delta_range_in;

// 1
call divstep_ver_a(x7, x8, x3, delta_range; x7, x8 ,x3, ne, delta_range);
// 2
call divstep_ver_b(x7, x8, x3, ne, delta_range; x7, x8 ,x3, ne, delta_range);
// 3
call divstep_ver_b(x7, x8, x3, ne, delta_range; x7, x8 ,x3, ne, delta_range);
// 4
call divstep_ver_b(x7, x8, x3, ne, delta_range; x7, x8 ,x3, ne, delta_range);
// 5
call divstep_ver_b(x7, x8, x3, ne, delta_range; x7, x8 ,x3, ne, delta_range);
// 6
call divstep_ver_b(x7, x8, x3, ne, delta_range; x7, x8 ,x3, ne, delta_range);
// 7
call divstep_ver_b(x7, x8, x3, ne, delta_range; x7, x8 ,x3, ne, delta_range);
// 8
call divstep_ver_b(x7, x8, x3, ne, delta_range; x7, x8 ,x3, ne, delta_range);
// 9
call divstep_ver_b(x7, x8, x3, ne, delta_range; x7, x8 ,x3, ne, delta_range);
// 10
call divstep_ver_b(x7, x8, x3, ne, delta_range; x7, x8 ,x3, ne, delta_range);
// 11
call divstep_ver_b(x7, x8, x3, ne, delta_range; x7, x8 ,x3, ne, delta_range);
// 12
call divstep_ver_b(x7, x8, x3, ne, delta_range; x7, x8 ,x3, ne, delta_range);
// 13
call divstep_ver_b(x7, x8, x3, ne, delta_range; x7, x8 ,x3, ne, delta_range);
// 14
call divstep_ver_b(x7, x8, x3, ne, delta_range; x7, x8 ,x3, ne, delta_range);
// 15
call divstep_ver_b(x7, x8, x3, ne, delta_range; x7, x8 ,x3, ne, delta_range);
// 16
call divstep_ver_b(x7, x8, x3, ne, delta_range; x7, x8 ,x3, ne, delta_range);
// 17
call divstep_ver_b(x7, x8, x3, ne, delta_range; x7, x8 ,x3, ne, delta_range);
// 18
call divstep_ver_b(x7, x8, x3, ne, delta_range; x7, x8 ,x3, ne, delta_range);
// 19
call divstep_ver_c(x7, x8, x3, ne, delta_range; x7, x8 ,x3, delta_range);

mov delta_range_out delta_range;
mov delta_out x3;

// from lemma we have
mov fuv x7;
mov grs x8;
nondet u_done@sint64;
nondet v_done@sint64;
nondet r_done@sint64;
nondet s_done@sint64;
nondet f_low60_40_low20_19@sint64;
nondet g_low60_40_low20_19@sint64;

assume 
u_done * f_low60_40_low20_0 + v_done * g_low60_40_low20_0 = f_low60_40_low20_19 * (-(2**20)),
r_done * f_low60_40_low20_0 + s_done * g_low60_40_low20_0 = g_low60_40_low20_19 * (-(2**20))
&&
fuv = f_low60_40_low20_19 + u_done * (const 64 (2**21)) + v_done * (const 64 (2**42)),
grs = g_low60_40_low20_19 + r_done * (const 64 (2**21)) + s_done * (const 64 (2**42)),
(const 64 (-(2**20)+1)) <=s f_low60_40_low20_19,
f_low60_40_low20_19 <=s (const 64 ((2**20)-1)),
(const 64 (-(2**20)+1)) <=s g_low60_40_low20_19,
g_low60_40_low20_19 <=s (const 64 ((2**20)-1)),
(const 64 (-(2**20))) <=s u_done, u_done <=s (const 64 ((2**20)-1)),
(const 64 (-(2**20))) <=s v_done, v_done <=s (const 64 ((2**20)-1)),
(const 64 (-(2**20))) <=s r_done, r_done <=s (const 64 ((2**20)-1)),
(const 64 (-(2**20))) <=s s_done, s_done <=s (const 64 ((2**20)-1)),
u_done + v_done <=s (const 64 (2**20)),
u_done - v_done <=s (const 64 (2**20)),
(const 64 0) - u_done + v_done <=s (const 64 (2**20)),
(const 64 0) - u_done - v_done <=s (const 64 (2**20)),
r_done + s_done <=s (const 64 (2**20)),
r_done - s_done <=s (const 64 (2**20)),
(const 64 0) - r_done + s_done <=s (const 64 (2**20)),
(const 64 0) - r_done - s_done <=s (const 64 (2**20))
;

// by the theory of jumpdivstep, we have
nondet f_low60_59@sint64;
nondet g_low60_59@sint64;

assume
u_done * f_low60_40 + v_done * g_low60_40 = f_low60_59 * (-(2**20)),
r_done * f_low60_40 + s_done * g_low60_40 = g_low60_59 * (-(2**20))
&&
f_low60_59 = (const 64 1) (mod (const 64 2))
;
// extraction
(* add	x12, x7, x6                                 #! PC = 0xaaaaafca0c08 *)
add x12 x7 x6;
(* asr	x12, x12, #42                               #! PC = 0xaaaaafca0c0c *)
cast x12@sint64 x12;
split x12 dc x12 42;
(* add	x11, x7, #0x100, lsl #12                    #! PC = 0xaaaaafca0c10 *)
add x11 x7 (2**20)@sint64;
(* lsl	x11, x11, #22                               #! PC = 0xaaaaafca0c14 *)
split dcH x11 x11 (64-22); shl x11 x11 22;
(* asr	x11, x11, #43                               #! PC = 0xaaaaafca0c18 *)
cast x11@sint64 x11;
split x11 dc x11 43;
(* add	x14, x8, x6                                 #! PC = 0xaaaaafca0c1c *)
add x14 x8 x6;
(* asr	x14, x14, #42                               #! PC = 0xaaaaafca0c20 *)
cast x14@sint64 x14;
split x14 dc x14 42;
(* add	x13, x8, #0x100, lsl #12                    #! PC = 0xaaaaafca0c24 *)
add x13 x8 (2**20)@sint64;
(* lsl	x13, x13, #22                               #! PC = 0xaaaaafca0c28 *)
split dcH x13 x13 (64-22); shl x13 x13 22;
(* asr	x13, x13, #43                               #! PC = 0xaaaaafca0c2c *)
cast x13@sint64 x13;
split x13 dc x13 43;

mov u x11;
mov v x12;
mov r x13;
mov s x14;
assert true && u = u_done;
assert true && v = v_done;
assert true && r = r_done;
assert true && s = s_done;
assume u = u_done && u = u_done;
assume v = v_done && v = v_done;
assume r = r_done && r = r_done;
assume s = s_done && s = s_done;

nondet neg_f_low60_59@sint64;
nondet neg_g_low60_59@sint64;
assume 
neg_f_low60_59 = - f_low60_59,
neg_g_low60_59 = - g_low60_59
&&
neg_f_low60_59 = (const 64 (-1)) * f_low60_59,
neg_g_low60_59 = (const 64 (-1)) * g_low60_59
;



// update_uuvvrrss
call update_uuvvrrss_2(
    u_in, v_in, r_in, s_in,
    u, v, r, s;
    u, v, r, s
);



assert 
u * f_low60_0 + v * g_low60_0 = neg_f_low60_59 * (2**60) (mod (2**64)),
r * f_low60_0 + s * g_low60_0 = neg_g_low60_59 * (2**60) (mod (2**64))
&& true;


{
u * f_low60_0 + v * g_low60_0 = neg_f_low60_59 * (2**60) (mod (2**64)),
r * f_low60_0 + s * g_low60_0 = neg_g_low60_59 * (2**60) (mod (2**64))
&&
neg_f_low60_59 = (1@sint64) (mod (2@sint64)),
(const 64 (-(2**60))) <=s u, u <=s (const 64 ((2**60)-1)),
(const 64 (-(2**60))) <=s v, v <=s (const 64 ((2**60)-1)),
(const 64 (-(2**60))) <=s r, r <=s (const 64 ((2**60)-1)),
(const 64 (-(2**60))) <=s s, s <=s (const 64 ((2**60)-1)),
u + v <=s (const 64 (2**60)),
u - v <=s (const 64 (2**60)),
(const 64 0) - u + v <=s (const 64 (2**60)),
(const 64 0) - u - v <=s (const 64 (2**60)),
r + s <=s (const 64 (2**60)),
r - s <=s (const 64 (2**60)),
(const 64 0) - r + s <=s (const 64 (2**60)),
(const 64 0) - r - s <=s (const 64 (2**60)),
((const 64 0)-delta_range_out) <=s delta_out, delta_out <=s delta_range_out,
delta_range_out = delta_range_in + (const 64 (2*19))
}


proc main()
={true && true}

nondet x1@sint64;
assume true && x1 = (1@sint64) (mod (2@sint64));
nondet x2@sint64;

mov f_low60_0 x1;
mov g_low60_0 x2;
mov x6 (2**20 + 2**41)@sint64;
mov x3 1@sint64;
mov delta_range 1@sint64;

call divstepx20_1(
    x1, x2, x6, x3, delta_range; x1, x2, x11, x12, x13, x14, x3, delta_range
);



call divstepx20_2(
    x1, x2, f_low60_0, g_low60_0, x11, x12, x13, x14, x6, x3, delta_range;
    x1, x2, x11, x12, x13, x14, x3, delta_range
);



call divstepx19_3(
    x1, x2, f_low60_0, g_low60_0, x11, x12, x13, x14, x6, x3, delta_range;
    neg_f_low60_59, neg_g_low60_59, x11, x12, x13, x14, x3, delta_range
);

assert true &&
x1 = 1@sint64 (mod (2@sint64)),
x6 = (2**20 + 2**41)@sint64,
((const 64 0)-delta_range) <=s x3, x3 <=s delta_range
// delta_range_in >=s (const 64 0),
// delta_range_in <=s 1300@sint64
;

mov u_0_59 x11;
mov v_0_59 x12;
mov r_0_59 x13;
mov s_0_59 x14;

assert 
u_0_59 * f_low60_0 + v_0_59 * g_low60_0 = neg_f_low60_59 * (2**60) (mod (2**64)),
r_0_59 * f_low60_0 + s_0_59 * g_low60_0 = neg_g_low60_59 * (2**60) (mod (2**64))
&& true;

// by the theory of jumpdivstep, we have
nondet f_0@sint64;
nondet g_0@sint64;
nondet neg_f_59@sint64;
nondet neg_g_59@sint64;
assume
    u_0_59 * f_0 + v_0_59 * g_0 = neg_f_59 * ((2**60)),
    r_0_59 * f_0 + s_0_59 * g_0 = neg_g_59 * ((2**60))
&&
neg_f_59 = (1@sint64) (mod (2@sint64))
;


and neg_f_59_low60_0@sint64 neg_f_59 (2**20 - 1)@sint64;
and neg_g_59_low60_0@sint64 neg_g_59 (2**20 - 1)@sint64;
mov x1 neg_f_59_low60_0;
mov x2 neg_g_59_low60_0;


call divstepx20_1(
    x1, x2, x6, x3, delta_range; x1, x2, x11, x12, x13, x14, x3, delta_range
);


call divstepx20_2(
    x1, x2, neg_f_59_low60_0, neg_g_59_low60_0, x11, x12, x13, x14, x6, x3, delta_range;
    x1, x2, x11, x12, x13, x14, x3, delta_range
);



call divstepx19_3(
    x1, x2, neg_f_59_low60_0, neg_g_59_low60_0, x11, x12, x13, x14, x6, x3, delta_range;
    f_59_low60_59, g_59_low60_59, x11, x12, x13, x14, x3, delta_range
);


mov u_59_118 x11;
mov v_59_118 x12;
mov r_59_118 x13;
mov s_59_118 x14;

assert 
u_59_118 * neg_f_59_low60_0 + v_59_118 * neg_g_59_low60_0 = f_59_low60_59 * (2**60) (mod (2**64)),
r_59_118 * neg_f_59_low60_0 + s_59_118 * neg_g_59_low60_0 = g_59_low60_59 * (2**60) (mod (2**64))
&& true;

// by the theory of jumpdivstep, we have
nondet f_118@sint64;
nondet g_118@sint64;
assume
    u_59_118 * neg_f_59 + v_59_118 * neg_g_59 = f_118 * ((2**60)),
    r_59_118 * neg_f_59 + s_59_118 * neg_g_59 = g_118 * ((2**60))
&& 
f_118 = (1@sint64) (mod (2@sint64))
;

{true && true}
