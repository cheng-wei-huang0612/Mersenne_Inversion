
proc one_side(
    f_0@uint64, f_1@uint64,
    g_0@uint64, g_1@uint64,
    u_or_r@sint64, v_or_s@sint64,
    sign_u_or_r@sint64, sign_v_or_s@sint64,
    abs_u_or_r@uint64, abs_v_or_s@uint64,
    new_f_or_g_expected@sint64;
    new_f_or_g@sint64
) = {
u_or_r * (f_0 + f_1 * (2**64)) + v_or_s * (g_0 + g_1 * (2**64)) = new_f_or_g_expected * (-(2**60)) (mod 2**128),
(sign_u_or_r * 2 + 1) * abs_u_or_r = u_or_r,
(sign_v_or_s * 2 + 1) * abs_v_or_s = v_or_s
&&
(const 64 (-(2**60))) <=s u_or_r, u_or_r <=s (const 64 ((2**60) - 1)),
(const 64 (-(2**60))) <=s v_or_s, v_or_s <=s (const 64 ((2**60) - 1)),
abs_u_or_r >=s (const 64 0),
abs_v_or_s >=s (const 64 0),
(sign_u_or_r * (const 64 2) + (const 64 1)) * abs_u_or_r = u_or_r,
(sign_v_or_s * (const 64 2) + (const 64 1)) * abs_v_or_s = v_or_s,
or [abs_u_or_r = u_or_r, abs_u_or_r = (const 64 0) - u_or_r],
or [abs_v_or_s = v_or_s, abs_v_or_s = (const 64 0) - v_or_s],
or [sign_u_or_r = 0@sint64, sign_u_or_r = (-1)@sint64],
or [sign_v_or_s = 0@sint64, sign_v_or_s = (-1)@sint64]
}



mov x4 f_0;
mov x21 f_1;
mov x5 g_0;
mov x22 g_1;
mov x11 abs_u_or_r;
mov x12 abs_v_or_s;
mov x23 sign_u_or_r;
mov x24 sign_v_or_s;


(* and	x27, x11, x23 *)
and x27@uint64 x11 x23;
(* and	x28, x12, x24                               #! PC = 0xaaaabe3719b8 *)
and x28@uint64 x12 x24;
(* add	x15, x27, x28                               #! PC = 0xaaaabe3719bc *)
add x15 x27 x28;


assert true &&
uext x15 64 = 
(uext abs_u_or_r 64) * ((const 128 0) - (sext sign_u_or_r 64)) +
(uext abs_v_or_s 64) * ((const 128 0) - (sext sign_v_or_s 64))
;

// assert true &&
// uext x15 64 = 
// (uext abs_u 64) * ((const 128 0) - ((uext sign_u 64) + (uext sign_u 64) * (const 128 (2**64)))) +
// (uext abs_v 64) * ((const 128 0) - ((uext sign_v 64) + (uext sign_v 64) * (const 128 (2**64))))
// ;




(* eor	x27, x4, x23                                #! PC = 0xaaaabe3719c0 *)
xor x27@uint64 x4 x23;


assert true && 
((sign_u_or_r * (const 64 2)) + (const 64 1)) * f_0 + sign_u_or_r = x27;
assume (sign_u_or_r * 2 + 1) * f_0 + sign_u_or_r = x27 &&
((sign_u_or_r * (const 64 2)) + (const 64 1)) * f_0 + sign_u_or_r = x27;

(* mul	x9, x27, x11                                #! PC = 0xaaaabe3719c4 *)
(* umulh	x10, x27, x11                             #! PC = 0xaaaabe3719c8 *)
mull x10 x9 x27 x11;


assert
limbs 64 [x9, x10] = x27 * x11
&& true;
assume 
limbs 64 [x9, x10] = x27 * x11
&&
limbs 64 [x9, x10] = 
((((uext sign_u_or_r 64) * (const 128 2)) + (const 128 1)) * (uext f_0 64) + (uext sign_u_or_r 64)) * (uext abs_u_or_r 64);



ghost x15_old@uint64, x16_old@uint64 :
    x15_old = x15, x16_old = 0
&&
    x15_old = x15, x16_old = 0@uint64;

(* adds	x15, x9, x15                               #! PC = 0xaaaabe3719cc *)
adds carry x15 x9 x15;
(* adc	x16, x10, xzr                               #! PC = 0xaaaabe3719d0 *)
adc x16 x10 0@uint64 carry;


// assert true && 
// limbs 64 [x15, x16] =
// limbs 64 [x9, x10] + limbs 64 [x15_old, x16_old]
// ;
//
// assert true &&
// limbs 64 [x15, x16] =
// (((((uext sign_u 64) * (const 128 2)) + (const 128 1)) * (uext f_0 64) + (uext sign_u 64)) * (uext abs_u 64)) +
// (uext x15_old 64);


assert true &&
limbs 64 [x15, x16] =
(((((uext sign_u_or_r 64) * (const 128 2)) + (const 128 1)) * (uext f_0 64) + (uext sign_u_or_r 64)) * (uext abs_u_or_r 64)) +
((uext abs_u_or_r 64) * ((const 128 0) - ((uext sign_u_or_r 64) + (uext sign_u_or_r 64) * (const 128 (2**64)))) +
(uext abs_v_or_s 64) * ((const 128 0) - ((uext sign_v_or_s 64) + (uext sign_v_or_s 64) * (const 128 (2**64)))))
;

assume
limbs 64 [x15, x16] =
((2 * sign_u_or_r + 1) * f_0 + sign_u_or_r) * abs_u_or_r +
abs_u_or_r * ( -(sign_u_or_r + sign_u_or_r * (2**64)) ) +
abs_v_or_s * ( -(sign_v_or_s + sign_v_or_s * (2**64)) )
(mod (2**128))
&&
limbs 64 [x15, x16] =
(((((uext sign_u_or_r 64) * (const 128 2)) + (const 128 1)) * (uext f_0 64) + (uext sign_u_or_r 64)) * (uext abs_u_or_r 64)) +
((uext abs_u_or_r 64) * ((const 128 0) - ((uext sign_u_or_r 64) + (uext sign_u_or_r 64) * (const 128 (2**64)))) +
(uext abs_v_or_s 64) * ((const 128 0) - ((uext sign_v_or_s 64) + (uext sign_v_or_s 64) * (const 128 (2**64)))))
;

assert
limbs 64 [x15, x16] =
((2 * sign_u_or_r + 1) * f_0) * abs_u_or_r +
abs_u_or_r * ( -(sign_u_or_r * (2**64)) ) +
abs_v_or_s * ( -(sign_v_or_s + sign_v_or_s * (2**64)) )
(mod (2**128))
&& true;

assume
limbs 64 [x15, x16] =
((2 * sign_u_or_r + 1) * f_0) * abs_u_or_r +
abs_u_or_r * ( -(sign_u_or_r * (2**64)) ) +
abs_v_or_s * ( -(sign_v_or_s + sign_v_or_s * (2**64)) )
(mod (2**128))
&&
limbs 64 [x15, x16] =
(((((uext sign_u_or_r 64) * (const 128 2)) + (const 128 1)) * (uext f_0 64)) * (uext abs_u_or_r 64)) +
((uext abs_u_or_r 64) * ((const 128 0) - ((uext sign_u_or_r 64) * (const 128 (2**64)))) +
(uext abs_v_or_s 64) * ((const 128 0) - ((uext sign_v_or_s 64) + (uext sign_v_or_s 64) * (const 128 (2**64)))))
;

(* eor	x27, x21, x23                               #! PC = 0xaaaabe3719d4 *)
xor x27@uint64 x21 x23;

assert true && 
((sign_u_or_r * (const 64 2)) + (const 64 1)) * f_1 + sign_u_or_r = x27;
assume (sign_u_or_r * 2 + 1) * f_1 + sign_u_or_r = x27 &&
((sign_u_or_r* (const 64 2)) + (const 64 1)) * f_1 + sign_u_or_r = x27;

(* mul	x9, x27, x11                                #! PC = 0xaaaabe3719d8 *)
mull dcH x9 x27 x11;

assert 
limbs 64 [0, x9] = x27 * x11 * (2**64) (mod (2**128))
&& true;

assume
limbs 64 [0, x9] = 
x27 * x11 * (2**64)
(mod (2**128))
&&
(const 128 (2**64)) * (uext x9 64) =
(uext x27 64) * (uext x11 64) * (const 128 (2**64))
;


// assert
// limbs 64 [0, x9] = 
// ((sign_u * 2 + 1) * f_1 + sign_u) * abs_u * (2**64)
// (mod (2**128))
// && true ;




ghost x15_old_1@uint64, x16_old_1@uint64 :
    x15_old_1 = x15, x16_old_1 = x16
&&
    x15_old_1 = x15, x16_old_1 = x16
;

(* add	x16, x16, x9                                #! PC = 0xaaaabe3719dc *)
adds dc x16 x16 x9;

assert true &&
limbs 64 [x15, x16] = (limbs 64 [x15_old_1, x16_old_1]) + (limbs 64 [(const 64 0), x9]);
    assume
    limbs 64 [x15, x16] = (limbs 64 [x15_old_1, x16_old_1]) + (limbs 64 [0, x9]) (mod (2**128))
    &&
    limbs 64 [x15, x16] = (limbs 64 [x15_old_1, x16_old_1]) + (limbs 64 [(const 64 0), x9]);

assert 
limbs 64 [x15, x16] = 
((2 * sign_u_or_r + 1) * f_0) * abs_u_or_r +
abs_u_or_r * ( -(sign_u_or_r * (2**64)) ) +
abs_v_or_s * ( -(sign_v_or_s + sign_v_or_s * (2**64)) )
+
((sign_u_or_r * 2 + 1) * f_1 + sign_u_or_r) * abs_u_or_r * (2**64)
(mod (2**128))
&& true;

cut 
limbs 64 [x15, x16] = 
(abs_u_or_r * (2 * sign_u_or_r + 1)) * (f_0 + f_1 * (2**64)) +
abs_v_or_s * ( -(sign_v_or_s + sign_v_or_s * (2**64)) )
(mod (2**128)),
(sign_u_or_r * 2 + 1) * abs_u_or_r = u_or_r,
(sign_v_or_s * 2 + 1) * abs_v_or_s = v_or_s,
    u_or_r * (f_0 + f_1 * (2**64)) + v_or_s * (g_0 + g_1 * (2**64)) = new_f_or_g_expected * (-(2**60)) (mod 2**128)
&&
or [sign_u_or_r = 0@sint64, sign_u_or_r = (-1)@sint64],
or [sign_v_or_s = 0@sint64, sign_v_or_s = (-1)@sint64]
;


assert
limbs 64 [x15, x16] = 
(u_or_r) * (f_0 + f_1 * (2**64)) +
abs_v_or_s * ( -(sign_v_or_s + sign_v_or_s * (2**64)) )
(mod (2**128))
&& true;



(* eor	x27, x5, x24                                #! PC = 0xaaaabe3719e0 *)
xor x27@uint64 x5 x24;

assert true && 
((sign_v_or_s * (const 64 2)) + (const 64 1)) * g_0 + sign_v_or_s = x27;
assume (sign_v_or_s * 2 + 1) * g_0 + sign_v_or_s = x27 &&
((sign_v_or_s * (const 64 2)) + (const 64 1)) * g_0 + sign_v_or_s = x27;

(* mul	x9, x27, x12                                #! PC = 0xaaaabe3719e4 *)
(* umulh	x10, x27, x12                             #! PC = 0xaaaabe3719e8 *)
mull x10 x9 x27 x12;

assert
limbs 64 [x9, x10] =
((sign_v_or_s * 2 + 1) * g_0 + sign_v_or_s) * abs_v_or_s
(mod (2**128))
&& true;

assume
limbs 64 [x9, x10] =
((sign_v_or_s * 2 + 1) * g_0 + sign_v_or_s) * abs_v_or_s
(mod (2**128))
&&
limbs 64 [x9, x10] =
(((uext sign_v_or_s 64) * (2@128) + 1@128) * (uext g_0 64) + (uext sign_v_or_s 64)) * (uext abs_v_or_s 64)
;


ghost x15_old_2@uint64, x16_old_2@uint64 :
    x15_old_2 = x15, x16_old_2 = x16
&&
    x15_old_2 = x15, x16_old_2 = x16
;

(* adds	x15, x9, x15                               #! PC = 0xaaaabe3719ec *)
adds carry x15 x9 x15;

(* adc	x16, x10, x16                               #! PC = 0xaaaabe3719f0 *)
adcs dc x16 x10 x16 carry;


assert true && 
limbs 64 [x15, x16] =
limbs 64 [x9, x10] + limbs 64 [x15_old_2, x16_old_2]
;
assume
limbs 64 [x15, x16] =
limbs 64 [x9, x10] + limbs 64 [x15_old_2, x16_old_2]
&&
limbs 64 [x15, x16] =
limbs 64 [x9, x10] + limbs 64 [x15_old_2, x16_old_2]
;

assert 
limbs 64 [x15, x16] =
((sign_v_or_s * 2 + 1) * g_0 + sign_v_or_s) * abs_v_or_s
+
(abs_u_or_r * (2 * sign_u_or_r + 1)) * (f_0 + f_1 * (2**64)) +
abs_v_or_s * ( -(sign_v_or_s + sign_v_or_s * (2**64)) )
(mod (2**128))
,
limbs 64 [x15, x16] =
((sign_v_or_s * 2 + 1) * g_0) * abs_v_or_s
+
(abs_u_or_r * (2 * sign_u_or_r + 1)) * (f_0 + f_1 * (2**64)) +
abs_v_or_s * ( -(sign_v_or_s * (2**64)) )
(mod (2**128))
&& true;


(* eor	x27, x22, x24                               #! PC = 0xaaaabe3719f4 *)
xor x27@uint64 x22 x24;


// XXX: 18 secs
assert true && 
((sign_v_or_s * (const 64 2)) + (const 64 1)) * g_1 + sign_v_or_s = x27;
assume (sign_v_or_s * 2 + 1) * g_1 + sign_v_or_s = x27 &&
((sign_v_or_s * (const 64 2)) + (const 64 1)) * g_1 + sign_v_or_s = x27;

(* mul	x9, x27, x12                                #! PC = 0xaaaabe3719f8 *)
mull dcH x9 x27 x12;

assert x9 * (2**64) = x27 * x12 * (2**64)
(mod (2**128))
&& true;

ghost x15_old_3@uint64, x16_old_3@uint64 :
    x15_old_3 = x15, x16_old_3 = x16
&&
    x15_old_3 = x15, x16_old_3 = x16
;

(* add	x16, x16, x9                                #! PC = 0xaaaabe3719fc *)
adds dc x16 x16 x9;

assert 
limbs 64 [x15, x16] =
(abs_u_or_r * (2 * sign_u_or_r + 1)) * (f_0 + f_1 * (2**64)) +
(abs_v_or_s * (2 * sign_v_or_s + 1)) * (g_0 + g_1 * (2**64))
(mod (2**128))
&& true;

assert 
limbs 64 [x15, x16] =
(u_or_r * (f_0 + f_1 * (2**64))) +
(v_or_s * (g_0 + g_1 * (2**64)))
(mod (2**128))
&& true;

cut
limbs 64 [x15, x16] =
(u_or_r * (f_0 + f_1 * (2**64))) +
(v_or_s * (g_0 + g_1 * (2**64)))
(mod (2**128)),
(sign_u_or_r * 2 + 1) * abs_u_or_r = u_or_r,
(sign_v_or_s * 2 + 1) * abs_v_or_s = v_or_s,
    u_or_r * (f_0 + f_1 * (2**64)) + v_or_s * (g_0 + g_1 * (2**64)) = new_f_or_g_expected * (-(2**60)) (mod 2**128)
&&
or [sign_u_or_r = 0@sint64, sign_u_or_r = (-1)@sint64],
or [sign_v_or_s = 0@sint64, sign_v_or_s = (-1)@sint64]
;


assert 
limbs 64 [x15, x16] =
new_f_or_g_expected * (-(2**60))
(mod (2**128))
&& true;

assume
limbs 64 [x15, x16] =
new_f_or_g_expected * (-(2**60))
(mod (2**128))
&& 
limbs 64 [x15, x16] =
(sext new_f_or_g_expected 64) * (const 128 (-(2**60)))
;


(* extr	x1, x16, x15, #60                          #! PC = 0xaaaabe371a00 *)
spl dc extr_H x16 60; spl extr_L dc x15 60; join x1 extr_H extr_L;

assert true &&
x1 = (const 64 (-1)) * new_f_or_g_expected (mod (const 64 (2**60)))
;

assume 
x1 = - new_f_or_g_expected (mod (2**60))
&&
x1 = (const 64 (-1)) * new_f_or_g_expected (mod (const 64 (2**60)))
;

mov new_f_or_g x1;


{
new_f_or_g = - new_f_or_g_expected (mod (2**60))
    &&
new_f_or_g = (const 64 (-1)) * new_f_or_g_expected (mod (const 64 (2**60)))
}

proc main (
 f_0@uint64, f_1@uint64,
 g_0@uint64, g_1@uint64,
 u@sint64, v@sint64, r@sint64, s@sint64,
 new_f_expected@sint64, new_g_expected@sint64;
 new_f@sint64, new_g@sint64
) = {
    u * (f_0 + f_1 * (2**64)) + v * (g_0 + g_1 * (2**64)) = new_f_expected * (-(2**60)) (mod 2**128),
    r * (f_0 + f_1 * (2**64)) + s * (g_0 + g_1 * (2**64)) = new_g_expected * (-(2**60)) (mod 2**128)
&&
(const 64 (-(2**60))) <=s u, u <=s (const 64 ((2**60) - 1)),
(const 64 (-(2**60))) <=s v, v <=s (const 64 ((2**60) - 1)),
(const 64 (-(2**60))) <=s r, r <=s (const 64 ((2**60) - 1)),
(const 64 (-(2**60))) <=s s, s <=s (const 64 ((2**60) - 1))
}

mov x4 f_0;
mov x21 f_1;
mov x5 g_0;
mov x22 g_1;
mov x11 u;
mov x12 v;
mov x13 r;
mov x14 s;



(* cmp	x11, xzr                                    #! PC = 0xaaaabe371984 *)
spl mi dc x11 63;
cast mi@bit mi;
(* csetm	x23, mi	// mi = first                     #! PC = 0xaaaabe371988 *)
cmov x23 mi (-1)@sint64 0@sint64;
(* cneg	x11, x11, mi	// mi = first                 #! PC = 0xaaaabe37198c *)
subs dc x11_neg 0@sint64 x11;
cmov x11 mi x11_neg x11;	// ge = tcont;;

(* cmp	x12, xzr                                    #! PC = 0xaaaabe371990 *)
spl mi dc x12 63;
cast mi@bit mi;
(* csetm	x24, mi	// mi = first                     #! PC = 0xaaaabe371994 *)
cmov x24 mi (-1)@sint64 0@sint64;
(* cneg	x12, x12, mi	// mi = first                 #! PC = 0xaaaabe371998 *)
subs dc x12_neg 0@sint64 x12;
cmov x12 mi x12_neg x12;	// ge = tcont;;

(* cmp	x13, xzr                                    #! PC = 0xaaaabe37199c *)
spl mi dc x13 63;
cast mi@bit mi;
(* csetm	x25, mi	// mi = first                     #! PC = 0xaaaabe3719a0 *)
cmov x25 mi (-1)@sint64 0@sint64;
(* cneg	x13, x13, mi	// mi = first                 #! PC = 0xaaaabe3719a4 *)
subs dc x13_neg 0@sint64 x13;
cmov x13 mi x13_neg x13;	// ge = tcont;;

(* cmp	x14, xzr                                    #! PC = 0xaaaabe3719a8 *)
spl mi dc x14 63;
cast mi@bit mi;
(* csetm	x26, mi	// mi = first                     #! PC = 0xaaaabe3719ac *)
cmov x26 mi (-1)@sint64 0@sint64;
(* cneg	x14, x14, mi	// mi = first                 #! PC = 0xaaaabe3719b0 *)
subs dc x14_neg 0@sint64 x14;
cmov x14 mi x14_neg x14;	// ge = tcont;;


mov sign_u x23;
mov sign_v x24;
mov sign_r x25;
mov sign_s x26;

cast x11@uint64 x11;
cast x12@uint64 x12;
cast x13@uint64 x13;
cast x14@uint64 x14;
mov abs_u x11;
mov abs_v x12;
mov abs_r x13;
mov abs_s x14;

assert true && 
x11 >=s (const 64 0),
x12 >=s (const 64 0),
x13 >=s (const 64 0),
x14 >=s (const 64 0);


assert true &&
or [abs_u = u, abs_u = (const 64 0) - u],
or [abs_v = v, abs_v = (const 64 0) - v],
or [abs_r = r, abs_r = (const 64 0) - r],
or [abs_s = s, abs_s = (const 64 0) - s],
or [sign_u = 0@sint64, sign_u = (-1)@sint64],
or [sign_v = 0@sint64, sign_v = (-1)@sint64],
or [sign_r = 0@sint64, sign_r = (-1)@sint64],
or [sign_s = 0@sint64, sign_s = (-1)@sint64];

assert true && (sign_u * (const 64 2) + (const 64 1)) * abs_u = u;
assert true && (sign_v * (const 64 2) + (const 64 1)) * abs_v = v;
assert true && (sign_r * (const 64 2) + (const 64 1)) * abs_r = r;
assert true && (sign_s * (const 64 2) + (const 64 1)) * abs_s = s;
assume (sign_u * 2 + 1) * abs_u = u && (sign_u * (const 64 2) + (const 64 1)) * abs_u = u;
assume (sign_v * 2 + 1) * abs_v = v && (sign_v * (const 64 2) + (const 64 1)) * abs_v = v;
assume (sign_r * 2 + 1) * abs_r = r && (sign_r * (const 64 2) + (const 64 1)) * abs_r = r;
assume (sign_s * 2 + 1) * abs_s = s && (sign_s * (const 64 2) + (const 64 1)) * abs_s = s;


call one_side(
    x4, x21,
    x5, x22,
    u, v,
    x23, x24,
    x11, x12,
    new_f_expected;
    x1
);

(* and	x27, x11, x23                               #! PC = 0xaaaabe3719b4 *)
and x27@uint64 x11 x23;
(* and	x28, x12, x24                               #! PC = 0xaaaabe3719b8 *)
and x28@uint64 x12 x24;
(* add	x15, x27, x28                               #! PC = 0xaaaabe3719bc *)
add x15 x27 x28;


// assert true &&
// uext x15 64 = 
// (uext abs_u 64) * ((const 128 0) - (sext sign_u 64)) +
// (uext abs_v 64) * ((const 128 0) - (sext sign_v 64))
// ;

// assert true &&
// uext x15 64 = 
// (uext abs_u 64) * ((const 128 0) - ((uext sign_u 64) + (uext sign_u 64) * (const 128 (2**64)))) +
// (uext abs_v 64) * ((const 128 0) - ((uext sign_v 64) + (uext sign_v 64) * (const 128 (2**64))))
// ;
//



(* eor	x27, x4, x23                                #! PC = 0xaaaabe3719c0 *)
xor x27@uint64 x4 x23;


assert true && 
((sign_u * (const 64 2)) + (const 64 1)) * f_0 + sign_u = x27;
assume (sign_u * 2 + 1) * f_0 + sign_u = x27 &&
((sign_u * (const 64 2)) + (const 64 1)) * f_0 + sign_u = x27;

(* mul	x9, x27, x11                                #! PC = 0xaaaabe3719c4 *)
(* umulh	x10, x27, x11                             #! PC = 0xaaaabe3719c8 *)
mull x10 x9 x27 x11;


assert
limbs 64 [x9, x10] = x27 * x11
&& true;
assume 
limbs 64 [x9, x10] = x27 * x11
&&
limbs 64 [x9, x10] = 
((((uext sign_u 64) * (const 128 2)) + (const 128 1)) * (uext f_0 64) + (uext sign_u 64)) * (uext abs_u 64);



ghost x15_old@uint64, x16_old@uint64 :
    x15_old = x15, x16_old = 0
&&
    x15_old = x15, x16_old = 0@uint64;

(* adds	x15, x9, x15                               #! PC = 0xaaaabe3719cc *)
adds carry x15 x9 x15;
(* adc	x16, x10, xzr                               #! PC = 0xaaaabe3719d0 *)
adc x16 x10 0@uint64 carry;


// assert true && 
// limbs 64 [x15, x16] =
// limbs 64 [x9, x10] + limbs 64 [x15_old, x16_old]
// ;
//
// assert true &&
// limbs 64 [x15, x16] =
// (((((uext sign_u 64) * (const 128 2)) + (const 128 1)) * (uext f_0 64) + (uext sign_u 64)) * (uext abs_u 64)) +
// (uext x15_old 64);


assert true &&
limbs 64 [x15, x16] =
(((((uext sign_u 64) * (const 128 2)) + (const 128 1)) * (uext f_0 64) + (uext sign_u 64)) * (uext abs_u 64)) +
((uext abs_u 64) * ((const 128 0) - ((uext sign_u 64) + (uext sign_u 64) * (const 128 (2**64)))) +
(uext abs_v 64) * ((const 128 0) - ((uext sign_v 64) + (uext sign_v 64) * (const 128 (2**64)))))
;

assume
limbs 64 [x15, x16] =
((2 * sign_u + 1) * f_0 + sign_u) * abs_u +
abs_u * ( -(sign_u + sign_u * (2**64)) ) +
abs_v * ( -(sign_v + sign_v * (2**64)) )
(mod (2**128))
&&
limbs 64 [x15, x16] =
(((((uext sign_u 64) * (const 128 2)) + (const 128 1)) * (uext f_0 64) + (uext sign_u 64)) * (uext abs_u 64)) +
((uext abs_u 64) * ((const 128 0) - ((uext sign_u 64) + (uext sign_u 64) * (const 128 (2**64)))) +
(uext abs_v 64) * ((const 128 0) - ((uext sign_v 64) + (uext sign_v 64) * (const 128 (2**64)))))
;

assert
limbs 64 [x15, x16] =
((2 * sign_u + 1) * f_0) * abs_u +
abs_u * ( -(sign_u * (2**64)) ) +
abs_v * ( -(sign_v + sign_v * (2**64)) )
(mod (2**128))
&& true;

assume
limbs 64 [x15, x16] =
((2 * sign_u + 1) * f_0) * abs_u +
abs_u * ( -(sign_u * (2**64)) ) +
abs_v * ( -(sign_v + sign_v * (2**64)) )
(mod (2**128))
&&
limbs 64 [x15, x16] =
(((((uext sign_u 64) * (const 128 2)) + (const 128 1)) * (uext f_0 64)) * (uext abs_u 64)) +
((uext abs_u 64) * ((const 128 0) - ((uext sign_u 64) * (const 128 (2**64)))) +
(uext abs_v 64) * ((const 128 0) - ((uext sign_v 64) + (uext sign_v 64) * (const 128 (2**64)))))
;

(* eor	x27, x21, x23                               #! PC = 0xaaaabe3719d4 *)
xor x27@uint64 x21 x23;

assert true && 
((sign_u * (const 64 2)) + (const 64 1)) * f_1 + sign_u = x27;
assume (sign_u * 2 + 1) * f_1 + sign_u = x27 &&
((sign_u * (const 64 2)) + (const 64 1)) * f_1 + sign_u = x27;

(* mul	x9, x27, x11                                #! PC = 0xaaaabe3719d8 *)
mull dcH x9 x27 x11;

assert 
limbs 64 [0, x9] = x27 * x11 * (2**64) (mod (2**128))
&& true;

assume
limbs 64 [0, x9] = 
x27 * x11 * (2**64)
(mod (2**128))
&&
(const 128 (2**64)) * (uext x9 64) =
(uext x27 64) * (uext x11 64) * (const 128 (2**64))
;


// assert
// limbs 64 [0, x9] = 
// ((sign_u * 2 + 1) * f_1 + sign_u) * abs_u * (2**64)
// (mod (2**128))
// && true ;




ghost x15_old_1@uint64, x16_old_1@uint64 :
    x15_old_1 = x15, x16_old_1 = x16
&&
    x15_old_1 = x15, x16_old_1 = x16
;

(* add	x16, x16, x9                                #! PC = 0xaaaabe3719dc *)
adds dc x16 x16 x9;

assert true &&
limbs 64 [x15, x16] = (limbs 64 [x15_old_1, x16_old_1]) + (limbs 64 [(const 64 0), x9]);
    assume
    limbs 64 [x15, x16] = (limbs 64 [x15_old_1, x16_old_1]) + (limbs 64 [0, x9]) (mod (2**128))
    &&
    limbs 64 [x15, x16] = (limbs 64 [x15_old_1, x16_old_1]) + (limbs 64 [(const 64 0), x9]);

assert 
limbs 64 [x15, x16] = 
((2 * sign_u + 1) * f_0) * abs_u +
abs_u * ( -(sign_u * (2**64)) ) +
abs_v * ( -(sign_v + sign_v * (2**64)) )
+
((sign_u * 2 + 1) * f_1 + sign_u) * abs_u * (2**64)
(mod (2**128))
&& true;

cut 
limbs 64 [x15, x16] = 
(abs_u * (2 * sign_u + 1)) * (f_0 + f_1 * (2**64)) +
abs_v * ( -(sign_v + sign_v * (2**64)) )
(mod (2**128)),
(sign_u * 2 + 1) * abs_u = u,
(sign_v * 2 + 1) * abs_v = v,
(sign_r * 2 + 1) * abs_r = r,
(sign_s * 2 + 1) * abs_s = s,
    u * (f_0 + f_1 * (2**64)) + v * (g_0 + g_1 * (2**64)) = new_f_expected * (-(2**60)) (mod 2**128),
    r * (f_0 + f_1 * (2**64)) + s * (g_0 + g_1 * (2**64)) = new_g_expected * (-(2**60)) (mod 2**128)
&&
or [sign_u = 0@sint64, sign_u = (-1)@sint64],
or [sign_v = 0@sint64, sign_v = (-1)@sint64],
or [sign_r = 0@sint64, sign_r = (-1)@sint64],
or [sign_s = 0@sint64, sign_s = (-1)@sint64]
;


assert
limbs 64 [x15, x16] = 
(u) * (f_0 + f_1 * (2**64)) +
abs_v * ( -(sign_v + sign_v * (2**64)) )
(mod (2**128))
&& true;



(* eor	x27, x5, x24                                #! PC = 0xaaaabe3719e0 *)
xor x27@uint64 x5 x24;

assert true && 
((sign_v * (const 64 2)) + (const 64 1)) * g_0 + sign_v = x27;
assume (sign_v * 2 + 1) * g_0 + sign_v = x27 &&
((sign_v * (const 64 2)) + (const 64 1)) * g_0 + sign_v = x27;

(* mul	x9, x27, x12                                #! PC = 0xaaaabe3719e4 *)
(* umulh	x10, x27, x12                             #! PC = 0xaaaabe3719e8 *)
mull x10 x9 x27 x12;

assert
limbs 64 [x9, x10] =
((sign_v * 2 + 1) * g_0 + sign_v) * abs_v
(mod (2**128))
&& true;

assume
limbs 64 [x9, x10] =
((sign_v * 2 + 1) * g_0 + sign_v) * abs_v
(mod (2**128))
&&
limbs 64 [x9, x10] =
(((uext sign_v 64) * (2@128) + 1@128) * (uext g_0 64) + (uext sign_v 64)) * (uext abs_v 64)
;


ghost x15_old_2@uint64, x16_old_2@uint64 :
    x15_old_2 = x15, x16_old_2 = x16
&&
    x15_old_2 = x15, x16_old_2 = x16
;

(* adds	x15, x9, x15                               #! PC = 0xaaaabe3719ec *)
adds carry x15 x9 x15;

(* adc	x16, x10, x16                               #! PC = 0xaaaabe3719f0 *)
adcs dc x16 x10 x16 carry;


assert true && 
limbs 64 [x15, x16] =
limbs 64 [x9, x10] + limbs 64 [x15_old_2, x16_old_2]
;
assume
limbs 64 [x15, x16] =
limbs 64 [x9, x10] + limbs 64 [x15_old_2, x16_old_2]
&&
limbs 64 [x15, x16] =
limbs 64 [x9, x10] + limbs 64 [x15_old_2, x16_old_2]
;

assert 
limbs 64 [x15, x16] =
((sign_v * 2 + 1) * g_0 + sign_v) * abs_v
+
(abs_u * (2 * sign_u + 1)) * (f_0 + f_1 * (2**64)) +
abs_v * ( -(sign_v + sign_v * (2**64)) )
(mod (2**128))
,
limbs 64 [x15, x16] =
((sign_v * 2 + 1) * g_0) * abs_v
+
(abs_u * (2 * sign_u + 1)) * (f_0 + f_1 * (2**64)) +
abs_v * ( -(sign_v * (2**64)) )
(mod (2**128))
&& true;


(* eor	x27, x22, x24                               #! PC = 0xaaaabe3719f4 *)
xor x27@uint64 x22 x24;


// XXX: 18 secs
// assert true && 
// ((sign_v * (const 64 2)) + (const 64 1)) * g_1 + sign_v = x27;
assume (sign_v * 2 + 1) * g_1 + sign_v = x27 &&
((sign_v * (const 64 2)) + (const 64 1)) * g_1 + sign_v = x27;

(* mul	x9, x27, x12                                #! PC = 0xaaaabe3719f8 *)
mull dcH x9 x27 x12;

assert x9 * (2**64) = x27 * x12 * (2**64)
(mod (2**128))
&& true;

ghost x15_old_3@uint64, x16_old_3@uint64 :
    x15_old_3 = x15, x16_old_3 = x16
&&
    x15_old_3 = x15, x16_old_3 = x16
;

(* add	x16, x16, x9                                #! PC = 0xaaaabe3719fc *)
adds dc x16 x16 x9;

assert 
limbs 64 [x15, x16] =
(abs_u * (2 * sign_u + 1)) * (f_0 + f_1 * (2**64)) +
(abs_v * (2 * sign_v + 1)) * (g_0 + g_1 * (2**64))
(mod (2**128))
&& true;

assert 
limbs 64 [x15, x16] =
(u * (f_0 + f_1 * (2**64))) +
(v * (g_0 + g_1 * (2**64)))
(mod (2**128))
&& true;

cut
limbs 64 [x15, x16] =
(u * (f_0 + f_1 * (2**64))) +
(v * (g_0 + g_1 * (2**64)))
(mod (2**128)),
(sign_u * 2 + 1) * abs_u = u,
(sign_v * 2 + 1) * abs_v = v,
(sign_r * 2 + 1) * abs_r = r,
(sign_s * 2 + 1) * abs_s = s,
    u * (f_0 + f_1 * (2**64)) + v * (g_0 + g_1 * (2**64)) = new_f_expected * (-(2**60)) (mod 2**128),
    r * (f_0 + f_1 * (2**64)) + s * (g_0 + g_1 * (2**64)) = new_g_expected * (-(2**60)) (mod 2**128)
&&
or [sign_u = 0@sint64, sign_u = (-1)@sint64],
or [sign_v = 0@sint64, sign_v = (-1)@sint64],
or [sign_r = 0@sint64, sign_r = (-1)@sint64],
or [sign_s = 0@sint64, sign_s = (-1)@sint64]
;


assert 
limbs 64 [x15, x16] =
new_f_expected * (-(2**60))
(mod (2**128))
&& true;

assume
limbs 64 [x15, x16] =
new_f_expected * (-(2**60))
(mod (2**128))
&& 
limbs 64 [x15, x16] =
(sext new_f_expected 64) * (const 128 (-(2**60)))
;


(* extr	x1, x16, x15, #60                          #! PC = 0xaaaabe371a00 *)
spl dc extr_H x16 60; spl extr_L dc x15 60; join x1 extr_H extr_L;

assert true &&
x1 = (const 64 (-1)) * new_f_expected (mod (const 64 (2**60)))
;

assume 
x1 = - new_f_expected (mod (2**60))
&&
x1 = (const 64 (-1)) * new_f_expected (mod (const 64 (2**60)))
;


(* and	x27, x13, x25                               #! PC = 0xaaaabe371a04 *)
(* and	x28, x14, x26                               #! PC = 0xaaaabe371a08 *)
(* add	x17, x27, x28                               #! PC = 0xaaaabe371a0c *)
(* eor	x27, x4, x25                                #! PC = 0xaaaabe371a10 *)
(* mul	x9, x27, x13                                #! PC = 0xaaaabe371a14 *)
(* umulh	x10, x27, x13                             #! PC = 0xaaaabe371a18 *)
(* adds	x17, x9, x17                               #! PC = 0xaaaabe371a1c *)
(* adc	x20, x10, xzr                               #! PC = 0xaaaabe371a20 *)
(* eor	x27, x21, x25                               #! PC = 0xaaaabe371a24 *)
(* mul	x9, x27, x13                                #! PC = 0xaaaabe371a28 *)
(* add	x20, x20, x9                                #! PC = 0xaaaabe371a2c *)
(* eor	x27, x5, x26                                #! PC = 0xaaaabe371a30 *)
(* mul	x9, x27, x14                                #! PC = 0xaaaabe371a34 *)
(* umulh	x10, x27, x14                             #! PC = 0xaaaabe371a38 *)
(* adds	x17, x9, x17                               #! PC = 0xaaaabe371a3c *)
(* adc	x20, x10, x20                               #! PC = 0xaaaabe371a40 *)
(* eor	x27, x22, x26                               #! PC = 0xaaaabe371a44 *)
(* mul	x9, x27, x14                                #! PC = 0xaaaabe371a48 *)
(* add	x20, x20, x9                                #! PC = 0xaaaabe371a4c *)
(* extr	x2, x20, x17, #60                          #! PC = 0xaaaabe371a50 *)

// mov x4 f_0;
// mov x21 f_1;
// mov x5 g_0;
// mov x22 g_1;
// mov x11 u;
// mov x12 v;
// mov x13 r;
// mov x14 s;
// mov sign_u x23;
// mov sign_v x24;
// mov sign_r x25;
// mov sign_s x26;
//
// cast x11@uint64 x11;
// cast x12@uint64 x12;
// cast x13@uint64 x13;
// cast x14@uint64 x14;
// mov abs_u x11;
// mov abs_v x12;
// mov abs_r x13;
// mov abs_s x14;
// call one_side(
//     x4, x21,
//     x5, x22,
//     r, s,
//     x25, x26,
//     x13, x14,
//     new_g_expected;
//     x2
// );
