proc main(int64 f_0_low60_0_low20_0_1, int64 g_0_low60_0_low20_0_1, uint64 op_x0_0, int64 x6_2, int64 x7_3, int64 x8_2) =
{ (-1048576) * f_0_low60_0_low20_0_1 + 0 * g_0_low60_0_low20_0_1 = f_0_low60_0_low20_0_1 * (- (2 ** 20)) /\ 0 * f_0_low60_0_low20_0_1 + (-1048576) * g_0_low60_0_low20_0_1 = g_0_low60_0_low20_0_1 * (- (2 ** 20)) && and [x7_3 = add (add f_0_low60_0_low20_0_1 (mul (-1048576)@64 2097152@64)) (mul 0@64 4398046511104@64), x8_2 = add (add g_0_low60_0_low20_0_1 (mul 0@64 2097152@64)) (mul (-1048576)@64 4398046511104@64), smod (sub (uext x7_3 1) (uext 1@64 1)) (uext 2@64 1) = 0@65, 1@64 = 1@64, (-1048576)@64 = (-1048576)@64, 0@64 = 0@64, 0@64 = 0@64, (-1048576)@64 = (-1048576)@64, x6_2 = 2199024304128@64] }
spl dc_41 x8_lo_1 x8_2 1;
and ne_1@uint1 x8_lo_1 1@uint1;
cmov x10_1 ne_1 x7_3 0@int64;
spl ge_1 dc_42 1@int64 63;
not ge_2@uint1 ge_1;
cmov ge_3 ne_1 ge_2 0@uint1;
subc dc_43 x3_neg_1 0@int64 1@int64;
cmov x3_3 ge_3 x3_neg_1 1@int64;
subc dc_44 x10_neg_1 0@int64 x10_1;
cmov x10_2 ge_3 x10_neg_1 x10_1;
cmov x7_5 ge_3 x8_2 x7_3;
adds dc_45 x8_4 x8_2 x10_2;
add x3_4 x3_3 2@int64;
spl dc_46 x8_lo_2 x8_4 2;
spl x8_target_1 dc_47 x8_lo_2 1;
and ne_2@uint1 x8_target_1 1@uint1;
split x8_5 dc_48 x8_4 1;
assert true && or [and [smod (sub (uext x8_2 1) (uext 0@64 1)) (uext 2@64 1) = 0@65, x7_5 = x7_3, mul x8_5 2@64 = x8_2, x3_4 = add 2@64 1@64], and [smod (sub (uext x8_2 1) (uext 1@64 1)) (uext 2@64 1) = 0@65, 1@64 <s 0@64, x7_5 = x7_3, mul x8_5 2@64 = add x8_2 x7_3, x3_4 = add 2@64 1@64], and [smod (sub (uext x8_2 1) (uext 1@64 1)) (uext 2@64 1) = 0@65, 1@64 >=s 0@64, x7_5 = x8_2, mul x8_5 2@64 = sub x8_2 x7_3, x3_4 = sub 2@64 1@64]];
cmov x10_3 ne_2 x7_5 0@int64;
spl ge_4 dc_49 x3_4 63;
not ge_5@uint1 ge_4;
cmov ge_6 ne_2 ge_5 0@uint1;
subc dc_50 x3_neg_2 0@int64 x3_4;
cmov x3_6 ge_6 x3_neg_2 x3_4;
subc dc_51 x10_neg_2 0@int64 x10_3;
cmov x10_4 ge_6 x10_neg_2 x10_3;
cmov x7_7 ge_6 x8_5 x7_5;
adds dc_52 x8_7 x8_5 x10_4;
add x3_7 x3_6 2@int64;
spl dc_53 x8_lo_3 x8_7 2;
spl x8_target_2 dc_54 x8_lo_3 1;
and ne_3@uint1 x8_target_2 1@uint1;
split x8_8 dc_55 x8_7 1;
assert true && or [and [smod (sub (uext x8_5 1) (uext 0@64 1)) (uext 2@64 1) = 0@65, x7_7 = x7_5, mul x8_8 2@64 = x8_5, x3_7 = add 2@64 x3_4], and [smod (sub (uext x8_5 1) (uext 1@64 1)) (uext 2@64 1) = 0@65, x3_4 <s 0@64, x7_7 = x7_5, mul x8_8 2@64 = add x8_5 x7_5, x3_7 = add 2@64 x3_4], and [smod (sub (uext x8_5 1) (uext 1@64 1)) (uext 2@64 1) = 0@65, x3_4 >=s 0@64, x7_7 = x8_5, mul x8_8 2@64 = sub x8_5 x7_5, x3_7 = sub 2@64 x3_4]];
cmov x10_5 ne_3 x7_7 0@int64;
spl ge_7 dc_56 x3_7 63;
not ge_8@uint1 ge_7;
cmov ge_9 ne_3 ge_8 0@uint1;
subc dc_57 x3_neg_3 0@int64 x3_7;
cmov x3_9 ge_9 x3_neg_3 x3_7;
subc dc_58 x10_neg_3 0@int64 x10_5;
cmov x10_6 ge_9 x10_neg_3 x10_5;
cmov x7_9 ge_9 x8_8 x7_7;
adds dc_59 x8_10 x8_8 x10_6;
add x3_10 x3_9 2@int64;
spl dc_60 x8_lo_4 x8_10 2;
spl x8_target_3 dc_61 x8_lo_4 1;
and ne_4@uint1 x8_target_3 1@uint1;
split x8_11 dc_62 x8_10 1;
assert true && or [and [smod (sub (uext x8_8 1) (uext 0@64 1)) (uext 2@64 1) = 0@65, x7_9 = x7_7, mul x8_11 2@64 = x8_8, x3_10 = add 2@64 x3_7], and [smod (sub (uext x8_8 1) (uext 1@64 1)) (uext 2@64 1) = 0@65, x3_7 <s 0@64, x7_9 = x7_7, mul x8_11 2@64 = add x8_8 x7_7, x3_10 = add 2@64 x3_7], and [smod (sub (uext x8_8 1) (uext 1@64 1)) (uext 2@64 1) = 0@65, x3_7 >=s 0@64, x7_9 = x8_8, mul x8_11 2@64 = sub x8_8 x7_7, x3_10 = sub 2@64 x3_7]];
cmov x10_7 ne_4 x7_9 0@int64;
spl ge_10 dc_63 x3_10 63;
not ge_11@uint1 ge_10;
cmov ge_12 ne_4 ge_11 0@uint1;
subc dc_64 x3_neg_4 0@int64 x3_10;
cmov x3_12 ge_12 x3_neg_4 x3_10;
subc dc_65 x10_neg_4 0@int64 x10_7;
cmov x10_8 ge_12 x10_neg_4 x10_7;
cmov x7_11 ge_12 x8_11 x7_9;
adds dc_66 x8_13 x8_11 x10_8;
add x3_13 x3_12 2@int64;
spl dc_67 x8_lo_5 x8_13 2;
spl x8_target_4 dc_68 x8_lo_5 1;
and ne_5@uint1 x8_target_4 1@uint1;
split x8_14 dc_69 x8_13 1;
assert true && or [and [smod (sub (uext x8_11 1) (uext 0@64 1)) (uext 2@64 1) = 0@65, x7_11 = x7_9, mul x8_14 2@64 = x8_11, x3_13 = add 2@64 x3_10], and [smod (sub (uext x8_11 1) (uext 1@64 1)) (uext 2@64 1) = 0@65, x3_10 <s 0@64, x7_11 = x7_9, mul x8_14 2@64 = add x8_11 x7_9, x3_13 = add 2@64 x3_10], and [smod (sub (uext x8_11 1) (uext 1@64 1)) (uext 2@64 1) = 0@65, x3_10 >=s 0@64, x7_11 = x8_11, mul x8_14 2@64 = sub x8_11 x7_9, x3_13 = sub 2@64 x3_10]];
cmov x10_9 ne_5 x7_11 0@int64;
spl ge_13 dc_70 x3_13 63;
not ge_14@uint1 ge_13;
cmov ge_15 ne_5 ge_14 0@uint1;
subc dc_71 x3_neg_5 0@int64 x3_13;
cmov x3_15 ge_15 x3_neg_5 x3_13;
subc dc_72 x10_neg_5 0@int64 x10_9;
cmov x10_10 ge_15 x10_neg_5 x10_9;
cmov x7_13 ge_15 x8_14 x7_11;
adds dc_73 x8_16 x8_14 x10_10;
add x3_16 x3_15 2@int64;
spl dc_74 x8_lo_6 x8_16 2;
spl x8_target_5 dc_75 x8_lo_6 1;
and ne_6@uint1 x8_target_5 1@uint1;
split x8_17 dc_76 x8_16 1;
assert true && or [and [smod (sub (uext x8_14 1) (uext 0@64 1)) (uext 2@64 1) = 0@65, x7_13 = x7_11, mul x8_17 2@64 = x8_14, x3_16 = add 2@64 x3_13], and [smod (sub (uext x8_14 1) (uext 1@64 1)) (uext 2@64 1) = 0@65, x3_13 <s 0@64, x7_13 = x7_11, mul x8_17 2@64 = add x8_14 x7_11, x3_16 = add 2@64 x3_13], and [smod (sub (uext x8_14 1) (uext 1@64 1)) (uext 2@64 1) = 0@65, x3_13 >=s 0@64, x7_13 = x8_14, mul x8_17 2@64 = sub x8_14 x7_11, x3_16 = sub 2@64 x3_13]];
cmov x10_11 ne_6 x7_13 0@int64;
spl ge_16 dc_77 x3_16 63;
not ge_17@uint1 ge_16;
cmov ge_18 ne_6 ge_17 0@uint1;
subc dc_78 x3_neg_6 0@int64 x3_16;
cmov x3_18 ge_18 x3_neg_6 x3_16;
subc dc_79 x10_neg_6 0@int64 x10_11;
cmov x10_12 ge_18 x10_neg_6 x10_11;
cmov x7_15 ge_18 x8_17 x7_13;
adds dc_80 x8_19 x8_17 x10_12;
add x3_19 x3_18 2@int64;
spl dc_81 x8_lo_7 x8_19 2;
spl x8_target_6 dc_82 x8_lo_7 1;
and ne_7@uint1 x8_target_6 1@uint1;
split x8_20 dc_83 x8_19 1;
assert true && or [and [smod (sub (uext x8_17 1) (uext 0@64 1)) (uext 2@64 1) = 0@65, x7_15 = x7_13, mul x8_20 2@64 = x8_17, x3_19 = add 2@64 x3_16], and [smod (sub (uext x8_17 1) (uext 1@64 1)) (uext 2@64 1) = 0@65, x3_16 <s 0@64, x7_15 = x7_13, mul x8_20 2@64 = add x8_17 x7_13, x3_19 = add 2@64 x3_16], and [smod (sub (uext x8_17 1) (uext 1@64 1)) (uext 2@64 1) = 0@65, x3_16 >=s 0@64, x7_15 = x8_17, mul x8_20 2@64 = sub x8_17 x7_13, x3_19 = sub 2@64 x3_16]];
cmov x10_13 ne_7 x7_15 0@int64;
spl ge_19 dc_84 x3_19 63;
not ge_20@uint1 ge_19;
cmov ge_21 ne_7 ge_20 0@uint1;
subc dc_85 x3_neg_7 0@int64 x3_19;
cmov x3_21 ge_21 x3_neg_7 x3_19;
subc dc_86 x10_neg_7 0@int64 x10_13;
cmov x10_14 ge_21 x10_neg_7 x10_13;
cmov x7_17 ge_21 x8_20 x7_15;
adds dc_87 x8_22 x8_20 x10_14;
add x3_22 x3_21 2@int64;
spl dc_88 x8_lo_8 x8_22 2;
spl x8_target_7 dc_89 x8_lo_8 1;
and ne_8@uint1 x8_target_7 1@uint1;
split x8_23 dc_90 x8_22 1;
assert true && or [and [smod (sub (uext x8_20 1) (uext 0@64 1)) (uext 2@64 1) = 0@65, x7_17 = x7_15, mul x8_23 2@64 = x8_20, x3_22 = add 2@64 x3_19], and [smod (sub (uext x8_20 1) (uext 1@64 1)) (uext 2@64 1) = 0@65, x3_19 <s 0@64, x7_17 = x7_15, mul x8_23 2@64 = add x8_20 x7_15, x3_22 = add 2@64 x3_19], and [smod (sub (uext x8_20 1) (uext 1@64 1)) (uext 2@64 1) = 0@65, x3_19 >=s 0@64, x7_17 = x8_20, mul x8_23 2@64 = sub x8_20 x7_15, x3_22 = sub 2@64 x3_19]];
cmov x10_15 ne_8 x7_17 0@int64;
spl ge_22 dc_91 x3_22 63;
not ge_23@uint1 ge_22;
cmov ge_24 ne_8 ge_23 0@uint1;
subc dc_92 x3_neg_8 0@int64 x3_22;
cmov x3_24 ge_24 x3_neg_8 x3_22;
subc dc_93 x10_neg_8 0@int64 x10_15;
cmov x10_16 ge_24 x10_neg_8 x10_15;
cmov x7_19 ge_24 x8_23 x7_17;
adds dc_94 x8_25 x8_23 x10_16;
add x3_25 x3_24 2@int64;
spl dc_95 x8_lo_9 x8_25 2;
spl x8_target_8 dc_96 x8_lo_9 1;
and ne_9@uint1 x8_target_8 1@uint1;
split x8_26 dc_97 x8_25 1;
assert true && or [and [smod (sub (uext x8_23 1) (uext 0@64 1)) (uext 2@64 1) = 0@65, x7_19 = x7_17, mul x8_26 2@64 = x8_23, x3_25 = add 2@64 x3_22], and [smod (sub (uext x8_23 1) (uext 1@64 1)) (uext 2@64 1) = 0@65, x3_22 <s 0@64, x7_19 = x7_17, mul x8_26 2@64 = add x8_23 x7_17, x3_25 = add 2@64 x3_22], and [smod (sub (uext x8_23 1) (uext 1@64 1)) (uext 2@64 1) = 0@65, x3_22 >=s 0@64, x7_19 = x8_23, mul x8_26 2@64 = sub x8_23 x7_17, x3_25 = sub 2@64 x3_22]];
cmov x10_17 ne_9 x7_19 0@int64;
spl ge_25 dc_98 x3_25 63;
not ge_26@uint1 ge_25;
cmov ge_27 ne_9 ge_26 0@uint1;
subc dc_99 x3_neg_9 0@int64 x3_25;
cmov x3_27 ge_27 x3_neg_9 x3_25;
subc dc_100 x10_neg_9 0@int64 x10_17;
cmov x10_18 ge_27 x10_neg_9 x10_17;
cmov x7_21 ge_27 x8_26 x7_19;
adds dc_101 x8_28 x8_26 x10_18;
add x3_28 x3_27 2@int64;
spl dc_102 x8_lo_10 x8_28 2;
spl x8_target_9 dc_103 x8_lo_10 1;
and ne_10@uint1 x8_target_9 1@uint1;
split x8_29 dc_104 x8_28 1;
assert true && or [and [smod (sub (uext x8_26 1) (uext 0@64 1)) (uext 2@64 1) = 0@65, x7_21 = x7_19, mul x8_29 2@64 = x8_26, x3_28 = add 2@64 x3_25], and [smod (sub (uext x8_26 1) (uext 1@64 1)) (uext 2@64 1) = 0@65, x3_25 <s 0@64, x7_21 = x7_19, mul x8_29 2@64 = add x8_26 x7_19, x3_28 = add 2@64 x3_25], and [smod (sub (uext x8_26 1) (uext 1@64 1)) (uext 2@64 1) = 0@65, x3_25 >=s 0@64, x7_21 = x8_26, mul x8_29 2@64 = sub x8_26 x7_19, x3_28 = sub 2@64 x3_25]];
cmov x10_19 ne_10 x7_21 0@int64;
spl ge_28 dc_105 x3_28 63;
not ge_29@uint1 ge_28;
cmov ge_30 ne_10 ge_29 0@uint1;
subc dc_106 x3_neg_10 0@int64 x3_28;
cmov x3_30 ge_30 x3_neg_10 x3_28;
subc dc_107 x10_neg_10 0@int64 x10_19;
cmov x10_20 ge_30 x10_neg_10 x10_19;
cmov x7_23 ge_30 x8_29 x7_21;
adds dc_108 x8_31 x8_29 x10_20;
add x3_31 x3_30 2@int64;
spl dc_109 x8_lo_11 x8_31 2;
spl x8_target_10 dc_110 x8_lo_11 1;
and ne_11@uint1 x8_target_10 1@uint1;
split x8_32 dc_111 x8_31 1;
assert true && or [and [smod (sub (uext x8_29 1) (uext 0@64 1)) (uext 2@64 1) = 0@65, x7_23 = x7_21, mul x8_32 2@64 = x8_29, x3_31 = add 2@64 x3_28], and [smod (sub (uext x8_29 1) (uext 1@64 1)) (uext 2@64 1) = 0@65, x3_28 <s 0@64, x7_23 = x7_21, mul x8_32 2@64 = add x8_29 x7_21, x3_31 = add 2@64 x3_28], and [smod (sub (uext x8_29 1) (uext 1@64 1)) (uext 2@64 1) = 0@65, x3_28 >=s 0@64, x7_23 = x8_29, mul x8_32 2@64 = sub x8_29 x7_21, x3_31 = sub 2@64 x3_28]];
cmov x10_21 ne_11 x7_23 0@int64;
spl ge_31 dc_112 x3_31 63;
not ge_32@uint1 ge_31;
cmov ge_33 ne_11 ge_32 0@uint1;
subc dc_113 x3_neg_11 0@int64 x3_31;
cmov x3_33 ge_33 x3_neg_11 x3_31;
subc dc_114 x10_neg_11 0@int64 x10_21;
cmov x10_22 ge_33 x10_neg_11 x10_21;
cmov x7_25 ge_33 x8_32 x7_23;
adds dc_115 x8_34 x8_32 x10_22;
add x3_34 x3_33 2@int64;
spl dc_116 x8_lo_12 x8_34 2;
spl x8_target_11 dc_117 x8_lo_12 1;
and ne_12@uint1 x8_target_11 1@uint1;
split x8_35 dc_118 x8_34 1;
assert true && or [and [smod (sub (uext x8_32 1) (uext 0@64 1)) (uext 2@64 1) = 0@65, x7_25 = x7_23, mul x8_35 2@64 = x8_32, x3_34 = add 2@64 x3_31], and [smod (sub (uext x8_32 1) (uext 1@64 1)) (uext 2@64 1) = 0@65, x3_31 <s 0@64, x7_25 = x7_23, mul x8_35 2@64 = add x8_32 x7_23, x3_34 = add 2@64 x3_31], and [smod (sub (uext x8_32 1) (uext 1@64 1)) (uext 2@64 1) = 0@65, x3_31 >=s 0@64, x7_25 = x8_32, mul x8_35 2@64 = sub x8_32 x7_23, x3_34 = sub 2@64 x3_31]];
cmov x10_23 ne_12 x7_25 0@int64;
spl ge_34 dc_119 x3_34 63;
not ge_35@uint1 ge_34;
cmov ge_36 ne_12 ge_35 0@uint1;
subc dc_120 x3_neg_12 0@int64 x3_34;
cmov x3_36 ge_36 x3_neg_12 x3_34;
subc dc_121 x10_neg_12 0@int64 x10_23;
cmov x10_24 ge_36 x10_neg_12 x10_23;
cmov x7_27 ge_36 x8_35 x7_25;
adds dc_122 x8_37 x8_35 x10_24;
add x3_37 x3_36 2@int64;
spl dc_123 x8_lo_13 x8_37 2;
spl x8_target_12 dc_124 x8_lo_13 1;
and ne_13@uint1 x8_target_12 1@uint1;
split x8_38 dc_125 x8_37 1;
assert true && or [and [smod (sub (uext x8_35 1) (uext 0@64 1)) (uext 2@64 1) = 0@65, x7_27 = x7_25, mul x8_38 2@64 = x8_35, x3_37 = add 2@64 x3_34], and [smod (sub (uext x8_35 1) (uext 1@64 1)) (uext 2@64 1) = 0@65, x3_34 <s 0@64, x7_27 = x7_25, mul x8_38 2@64 = add x8_35 x7_25, x3_37 = add 2@64 x3_34], and [smod (sub (uext x8_35 1) (uext 1@64 1)) (uext 2@64 1) = 0@65, x3_34 >=s 0@64, x7_27 = x8_35, mul x8_38 2@64 = sub x8_35 x7_25, x3_37 = sub 2@64 x3_34]];
cmov x10_25 ne_13 x7_27 0@int64;
spl ge_37 dc_126 x3_37 63;
not ge_38@uint1 ge_37;
cmov ge_39 ne_13 ge_38 0@uint1;
subc dc_127 x3_neg_13 0@int64 x3_37;
cmov x3_39 ge_39 x3_neg_13 x3_37;
subc dc_128 x10_neg_13 0@int64 x10_25;
cmov x10_26 ge_39 x10_neg_13 x10_25;
cmov x7_29 ge_39 x8_38 x7_27;
adds dc_129 x8_40 x8_38 x10_26;
add x3_40 x3_39 2@int64;
spl dc_130 x8_lo_14 x8_40 2;
spl x8_target_13 dc_131 x8_lo_14 1;
and ne_14@uint1 x8_target_13 1@uint1;
split x8_41 dc_132 x8_40 1;
assert true && or [and [smod (sub (uext x8_38 1) (uext 0@64 1)) (uext 2@64 1) = 0@65, x7_29 = x7_27, mul x8_41 2@64 = x8_38, x3_40 = add 2@64 x3_37], and [smod (sub (uext x8_38 1) (uext 1@64 1)) (uext 2@64 1) = 0@65, x3_37 <s 0@64, x7_29 = x7_27, mul x8_41 2@64 = add x8_38 x7_27, x3_40 = add 2@64 x3_37], and [smod (sub (uext x8_38 1) (uext 1@64 1)) (uext 2@64 1) = 0@65, x3_37 >=s 0@64, x7_29 = x8_38, mul x8_41 2@64 = sub x8_38 x7_27, x3_40 = sub 2@64 x3_37]];
cmov x10_27 ne_14 x7_29 0@int64;
spl ge_40 dc_133 x3_40 63;
not ge_41@uint1 ge_40;
cmov ge_42 ne_14 ge_41 0@uint1;
subc dc_134 x3_neg_14 0@int64 x3_40;
cmov x3_42 ge_42 x3_neg_14 x3_40;
subc dc_135 x10_neg_14 0@int64 x10_27;
cmov x10_28 ge_42 x10_neg_14 x10_27;
cmov x7_31 ge_42 x8_41 x7_29;
adds dc_136 x8_43 x8_41 x10_28;
add x3_43 x3_42 2@int64;
spl dc_137 x8_lo_15 x8_43 2;
spl x8_target_14 dc_138 x8_lo_15 1;
and ne_15@uint1 x8_target_14 1@uint1;
split x8_44 dc_139 x8_43 1;
assert true && or [and [smod (sub (uext x8_41 1) (uext 0@64 1)) (uext 2@64 1) = 0@65, x7_31 = x7_29, mul x8_44 2@64 = x8_41, x3_43 = add 2@64 x3_40], and [smod (sub (uext x8_41 1) (uext 1@64 1)) (uext 2@64 1) = 0@65, x3_40 <s 0@64, x7_31 = x7_29, mul x8_44 2@64 = add x8_41 x7_29, x3_43 = add 2@64 x3_40], and [smod (sub (uext x8_41 1) (uext 1@64 1)) (uext 2@64 1) = 0@65, x3_40 >=s 0@64, x7_31 = x8_41, mul x8_44 2@64 = sub x8_41 x7_29, x3_43 = sub 2@64 x3_40]];
cmov x10_29 ne_15 x7_31 0@int64;
spl ge_43 dc_140 x3_43 63;
not ge_44@uint1 ge_43;
cmov ge_45 ne_15 ge_44 0@uint1;
subc dc_141 x3_neg_15 0@int64 x3_43;
cmov x3_45 ge_45 x3_neg_15 x3_43;
subc dc_142 x10_neg_15 0@int64 x10_29;
cmov x10_30 ge_45 x10_neg_15 x10_29;
cmov x7_33 ge_45 x8_44 x7_31;
adds dc_143 x8_46 x8_44 x10_30;
add x3_46 x3_45 2@int64;
spl dc_144 x8_lo_16 x8_46 2;
spl x8_target_15 dc_145 x8_lo_16 1;
and ne_16@uint1 x8_target_15 1@uint1;
split x8_47 dc_146 x8_46 1;
assert true && or [and [smod (sub (uext x8_44 1) (uext 0@64 1)) (uext 2@64 1) = 0@65, x7_33 = x7_31, mul x8_47 2@64 = x8_44, x3_46 = add 2@64 x3_43], and [smod (sub (uext x8_44 1) (uext 1@64 1)) (uext 2@64 1) = 0@65, x3_43 <s 0@64, x7_33 = x7_31, mul x8_47 2@64 = add x8_44 x7_31, x3_46 = add 2@64 x3_43], and [smod (sub (uext x8_44 1) (uext 1@64 1)) (uext 2@64 1) = 0@65, x3_43 >=s 0@64, x7_33 = x8_44, mul x8_47 2@64 = sub x8_44 x7_31, x3_46 = sub 2@64 x3_43]];
cmov x10_31 ne_16 x7_33 0@int64;
spl ge_46 dc_147 x3_46 63;
not ge_47@uint1 ge_46;
cmov ge_48 ne_16 ge_47 0@uint1;
subc dc_148 x3_neg_16 0@int64 x3_46;
cmov x3_48 ge_48 x3_neg_16 x3_46;
subc dc_149 x10_neg_16 0@int64 x10_31;
cmov x10_32 ge_48 x10_neg_16 x10_31;
cmov x7_35 ge_48 x8_47 x7_33;
adds dc_150 x8_49 x8_47 x10_32;
add x3_49 x3_48 2@int64;
spl dc_151 x8_lo_17 x8_49 2;
spl x8_target_16 dc_152 x8_lo_17 1;
and ne_17@uint1 x8_target_16 1@uint1;
split x8_50 dc_153 x8_49 1;
assert true && or [and [smod (sub (uext x8_47 1) (uext 0@64 1)) (uext 2@64 1) = 0@65, x7_35 = x7_33, mul x8_50 2@64 = x8_47, x3_49 = add 2@64 x3_46], and [smod (sub (uext x8_47 1) (uext 1@64 1)) (uext 2@64 1) = 0@65, x3_46 <s 0@64, x7_35 = x7_33, mul x8_50 2@64 = add x8_47 x7_33, x3_49 = add 2@64 x3_46], and [smod (sub (uext x8_47 1) (uext 1@64 1)) (uext 2@64 1) = 0@65, x3_46 >=s 0@64, x7_35 = x8_47, mul x8_50 2@64 = sub x8_47 x7_33, x3_49 = sub 2@64 x3_46]];
cmov x10_33 ne_17 x7_35 0@int64;
spl ge_49 dc_154 x3_49 63;
not ge_50@uint1 ge_49;
cmov ge_51 ne_17 ge_50 0@uint1;
subc dc_155 x3_neg_17 0@int64 x3_49;
cmov x3_51 ge_51 x3_neg_17 x3_49;
subc dc_156 x10_neg_17 0@int64 x10_33;
cmov x10_34 ge_51 x10_neg_17 x10_33;
cmov x7_37 ge_51 x8_50 x7_35;
adds dc_157 x8_52 x8_50 x10_34;
add x3_52 x3_51 2@int64;
spl dc_158 x8_lo_18 x8_52 2;
spl x8_target_17 dc_159 x8_lo_18 1;
and ne_18@uint1 x8_target_17 1@uint1;
split x8_53 dc_160 x8_52 1;
assert true && or [and [smod (sub (uext x8_50 1) (uext 0@64 1)) (uext 2@64 1) = 0@65, x7_37 = x7_35, mul x8_53 2@64 = x8_50, x3_52 = add 2@64 x3_49], and [smod (sub (uext x8_50 1) (uext 1@64 1)) (uext 2@64 1) = 0@65, x3_49 <s 0@64, x7_37 = x7_35, mul x8_53 2@64 = add x8_50 x7_35, x3_52 = add 2@64 x3_49], and [smod (sub (uext x8_50 1) (uext 1@64 1)) (uext 2@64 1) = 0@65, x3_49 >=s 0@64, x7_37 = x8_50, mul x8_53 2@64 = sub x8_50 x7_35, x3_52 = sub 2@64 x3_49]];
cmov x10_35 ne_18 x7_37 0@int64;
spl ge_52 dc_161 x3_52 63;
not ge_53@uint1 ge_52;
cmov ge_54 ne_18 ge_53 0@uint1;
subc dc_162 x3_neg_18 0@int64 x3_52;
cmov x3_54 ge_54 x3_neg_18 x3_52;
subc dc_163 x10_neg_18 0@int64 x10_35;
cmov x10_36 ge_54 x10_neg_18 x10_35;
cmov x7_39 ge_54 x8_53 x7_37;
adds dc_164 x8_55 x8_53 x10_36;
add x3_55 x3_54 2@int64;
spl dc_165 x8_lo_19 x8_55 2;
spl x8_target_18 dc_166 x8_lo_19 1;
and ne_19@uint1 x8_target_18 1@uint1;
split x8_56 dc_167 x8_55 1;
assert true && or [and [smod (sub (uext x8_53 1) (uext 0@64 1)) (uext 2@64 1) = 0@65, x7_39 = x7_37, mul x8_56 2@64 = x8_53, x3_55 = add 2@64 x3_52], and [smod (sub (uext x8_53 1) (uext 1@64 1)) (uext 2@64 1) = 0@65, x3_52 <s 0@64, x7_39 = x7_37, mul x8_56 2@64 = add x8_53 x7_37, x3_55 = add 2@64 x3_52], and [smod (sub (uext x8_53 1) (uext 1@64 1)) (uext 2@64 1) = 0@65, x3_52 >=s 0@64, x7_39 = x8_53, mul x8_56 2@64 = sub x8_53 x7_37, x3_55 = sub 2@64 x3_52]];
cmov x10_37 ne_19 x7_39 0@int64;
spl ge_55 dc_168 x3_55 63;
not ge_56@uint1 ge_55;
cmov ge_57 ne_19 ge_56 0@uint1;
subc dc_169 x3_neg_19 0@int64 x3_55;
cmov x3_57 ge_57 x3_neg_19 x3_55;
subc dc_170 x10_neg_19 0@int64 x10_37;
cmov x10_38 ge_57 x10_neg_19 x10_37;
cmov x7_41 ge_57 x8_56 x7_39;
adds dc_171 x8_58 x8_56 x10_38;
add x3_58 x3_57 2@int64;
spl dc_172 x8_lo_20 x8_58 2;
spl x8_target_19 dc_173 x8_lo_20 1;
and ne_20@uint1 x8_target_19 1@uint1;
split x8_59 dc_174 x8_58 1;
assert true && or [and [smod (sub (uext x8_56 1) (uext 0@64 1)) (uext 2@64 1) = 0@65, x7_41 = x7_39, mul x8_59 2@64 = x8_56, x3_58 = add 2@64 x3_55], and [smod (sub (uext x8_56 1) (uext 1@64 1)) (uext 2@64 1) = 0@65, x3_55 <s 0@64, x7_41 = x7_39, mul x8_59 2@64 = add x8_56 x7_39, x3_58 = add 2@64 x3_55], and [smod (sub (uext x8_56 1) (uext 1@64 1)) (uext 2@64 1) = 0@65, x3_55 >=s 0@64, x7_41 = x8_56, mul x8_59 2@64 = sub x8_56 x7_39, x3_58 = sub 2@64 x3_55]];
cmov x10_39 ne_20 x7_41 0@int64;
spl ge_58 dc_175 x3_58 63;
not ge_59@uint1 ge_58;
cmov ge_60 ne_20 ge_59 0@uint1;
subc dc_176 x3_neg_20 0@int64 x3_58;
cmov x3_60 ge_60 x3_neg_20 x3_58;
subc dc_177 x10_neg_20 0@int64 x10_39;
cmov x10_40 ge_60 x10_neg_20 x10_39;
cmov x7_43 ge_60 x8_59 x7_41;
adds dc_178 x8_61 x8_59 x10_40;
add x3_61 x3_60 2@int64;
split x8_62 dc_179 x8_61 1;
assert true && or [and [smod (sub (uext x8_59 1) (uext 0@64 1)) (uext 2@64 1) = 0@65, x7_43 = x7_41, mul x8_62 2@64 = x8_59, x3_61 = add 2@64 x3_58], and [smod (sub (uext x8_59 1) (uext 1@64 1)) (uext 2@64 1) = 0@65, x3_58 <s 0@64, x7_43 = x7_41, mul x8_62 2@64 = add x8_59 x7_41, x3_61 = add 2@64 x3_58], and [smod (sub (uext x8_59 1) (uext 1@64 1)) (uext 2@64 1) = 0@65, x3_58 >=s 0@64, x7_43 = x8_59, mul x8_62 2@64 = sub x8_59 x7_41, x3_61 = sub 2@64 x3_58]];
nondet f_0_low60_0_low20_20_1@int64;
nondet g_0_low60_0_low20_20_1@int64;
nondet u_2@int64;
nondet v_2@int64;
nondet r_2@int64;
nondet s_2@int64;
assume r_2 * f_0_low60_0_low20_0_1 + s_2 * g_0_low60_0_low20_0_1 = g_0_low60_0_low20_20_1 * (- (2 ** 20)) /\ u_2 * f_0_low60_0_low20_0_1 + v_2 * g_0_low60_0_low20_0_1 = f_0_low60_0_low20_20_1 * (- (2 ** 20)) && and [s_2 <=s 1048575@64, (-1048576)@64 <=s s_2, r_2 <=s 1048575@64, (-1048576)@64 <=s r_2, v_2 <=s 1048575@64, (-1048576)@64 <=s v_2, u_2 <=s 1048575@64, (-1048576)@64 <=s u_2, x3_61 <=s 41@64, (-39)@64 <=s x3_61, smod (sub (uext x3_61 1) (uext 1@64 1)) (uext 2@64 1) = 0@65, g_0_low60_0_low20_20_1 <=s 1048575@64, (-1048575)@64 <=s g_0_low60_0_low20_20_1, f_0_low60_0_low20_20_1 <=s 1048575@64, (-1048575)@64 <=s f_0_low60_0_low20_20_1, smod (sub (uext x7_43 1) (uext 1@64 1)) (uext 2@64 1) = 0@65, x8_62 = add (add g_0_low60_0_low20_20_1 (mul r_2 2097152@64)) (mul s_2 4398046511104@64), x7_43 = add (add f_0_low60_0_low20_20_1 (mul u_2 2097152@64)) (mul v_2 4398046511104@64)];
nondet f_0_low60_0_1@int64;
nondet g_0_low60_0_1@int64;
nondet f_0_low60_20_1@int64;
nondet g_0_low60_20_1@int64;
assume r_2 * 18446744073709551597 + s_2 * op_x0_0 = g_0_low60_20_1 * (- (2 ** 20)) /\ u_2 * 18446744073709551597 + v_2 * op_x0_0 = f_0_low60_20_1 * (- (2 ** 20)) && true;
{ u_2 * 18446744073709551597 + v_2 * op_x0_0 = f_0_low60_20_1 * (- (2 ** 20)) /\ r_2 * 18446744073709551597 + s_2 * op_x0_0 = g_0_low60_20_1 * (- (2 ** 20)) && and [x7_43 = add (add f_0_low60_0_low20_20_1 (mul u_2 2097152@64)) (mul v_2 4398046511104@64), x8_62 = add (add g_0_low60_0_low20_20_1 (mul r_2 2097152@64)) (mul s_2 4398046511104@64), (-1048575)@64 <=s f_0_low60_0_low20_20_1, f_0_low60_0_low20_20_1 <=s 1048575@64, (-1048575)@64 <=s g_0_low60_0_low20_20_1, g_0_low60_0_low20_20_1 <=s 1048575@64, smod (sub (uext x7_43 1) (uext 1@64 1)) (uext 2@64 1) = 0@65, smod (sub (uext f_0_low60_0_low20_20_1 1) (uext 1@64 1)) (uext 2@64 1) = 0@65, smod (sub (uext x3_61 1) (uext 1@64 1)) (uext 2@64 1) = 0@65, (-39)@64 <=s x3_61, x3_61 <=s 41@64, (-1048576)@64 <=s u_2, u_2 <=s 1048575@64, (-1048576)@64 <=s v_2, v_2 <=s 1048575@64, (-1048576)@64 <=s r_2, r_2 <=s 1048575@64, (-1048576)@64 <=s s_2, s_2 <=s 1048575@64, x6_2 = 2199024304128@64] }